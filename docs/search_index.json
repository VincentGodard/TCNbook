[["index.html", "Interpreting TCN concentrations at the Earth Surface Front page", " Interpreting TCN concentrations at the Earth Surface Vincent Godard 2022-10-18 Front page "],["introduction.html", "1 Introduction 1.1 Terrestrial Cosmogenic Nuclides and Earth Surface Dynamics 1.2 Scope and objectives 1.3 Working with this web site 1.4 Getting started with R/RStudio 1.5 Usage", " 1 Introduction 1.1 Terrestrial Cosmogenic Nuclides and Earth Surface Dynamics 1.2 Scope and objectives The objective of these activities is to allow users to explore a number of ideas and concepts related to the use of TCNs for the study of surface processes, in particular for exposure and denudation issues. The book is built from small sequences of numerical code allowing to produce the presented figures. These sequences are organized to be easily reusable and allow to quickly test the influence of different parameters on the calculations. 1.3 Working with this web site This html site was generated from R Markdown files using the bookdown package. You can copy/paste the various lines of code into your own R script and run it in any R session. These activities do not require any specific prior knowledge of R programming language. The idea is for you to simply copy and paste the code in a script, run it, and change various parameters to observe and investigate the associated response. In addition to the scripting-oriented activities below you can also experiment visually with a few interactive Shiny apps : http://shinyproxy.osupytheas.fr Both are built on the dedicated TCNtools package : https://vincentgodard.github.io/TCNtools This package contain various functions to assist the interpretation of TCN concentration at the Earth Surface. The first thing you will have to do, before calling any of the functions, is always to load the TCNtools package. Code library(&quot;TCNtools&quot;) But before that you need to have a R/RStudio working environment. 1.4 Getting started with R/RStudio Various options can be considered to run the R code bits presented in this book. 1.4.1 Local installation You can install RStudio directly on your system and then the TCNTools package using the instruction provided on the package webpage (requires devtools package to install from Github). Code install.packages(&quot;devtools&quot;) devtools::install_git(url = &quot;https://github.com/VincentGodard/TCNtools&quot;) Pros Run locally, not dependent on a distant server with possible usage limits Cons : Requires to install R/RStudio, which might not be interesting if you do not use them on a regular basis devtools install can sometimes take some work, in particular on Windows systems 1.4.2 Binder Alternatively you can launch a binder session and get a RStudio session running into your browser with the TCNtools package already installed, by clicking on the icon on the package Github repository (top of Readme.md): https://github.com/VincentGodard/TCNtools Pros Run into your browser Nothing to install, just one click Cons : Temporary session, you loose everything when closing the browser (but it is possible to download your files) Limit on the number of instances that can run simultaneously 1.4.3 RStudio Cloud Another option is to create a free account on https://rstudio.cloud. Then start a project and install the TCNTools package. Code install.packages(&quot;devtools&quot;) devtools::install_git(url = &quot;https://github.com/VincentGodard/TCNtools&quot;) Pros Run in your browser Simple installation Permanent account, allowing to keep projects and files from one session to the other Cons : A 25 hours per month limit with the free base plan. Remember to close you session when you are not working 1.5 Usage Various boxes will be used throughout the book. This a summary code box. In the following chapters the processes will be explained step by step, with the various actions distributed between different successive code blocks (also named chunks) for better understanding. For convenience, and avoid you to have to copy and paste these various blocks one by one, sometimes everything will be combined in one block at the end of the section, which can unfold, copy and then paste into the R console or a script. Below you can get all the necessary code to produce a figure or result. Code a = 1 b = 3 c = (a+b)^2 The purpose of this book is to allow you to experiment and explore, not just copy and paste some code. This kind of box will suggest you some action on the code, typically changing the value of parameters to observe the evolution of the result. This box will be used to highlight or stress something important. This box will be used to formulate some questions for you to think about. "],["scaling-factors.html", "2 Scaling factors 2.1 Time-independent scaling 2.2 Time-dependent scalings 2.3 Interactive shiny app", " 2 Scaling factors First we are going to explore how the scaling factors are changing with elevation, latitude and time, and what is the impact on local production rates. 2.1 Time-independent scaling We are going to present the most widely used and simplest scaling scheme known as Lal-Stone and often abbreviated as st. The main equations are presented in the reference article by Stone (2000) . 2.1.1 Site characteristics We first need to define some parameters concerning the site of interest : latitude lat in degrees altitude z in meters (can be a vector or a scalar) longitude lon in degrees, this is not used for st scaling (Stone (2000)), just in case we want to compute atmospheric pressure according to ERA40 (Uppala et al. (2005)). Code lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments Now we can compute the atmospheric pressure, with the function atm_pressure according to the two models available, and then plot for comparison. Here z is a vector to see the variations over a range of elevations. To get information about the usage of the function used here (for example what are the different models) type ?atm_pressure in the R console. Code P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) Finally, we plot the results. Code plot(P1,z,type=&quot;l&quot;,xlab=&quot;Pressure (hPa)&quot;,ylab=&quot;Altitude (m)&quot;,col=&quot;darkorange3&quot;) lines(P2,z,lty=2,col=&quot;darkorange3&quot;) legend(&quot;topright&quot;,c(&quot;Stone 2000&quot;,&quot;ERA40&quot;),lty=c(1,2)) Modify lat and lon to see the effects on the pressure computed with the ERA40 model. Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) plot(P1,z,type=&quot;l&quot;,xlab=&quot;Pressure (hPa)&quot;,ylab=&quot;Altitude (m)&quot;,col=&quot;darkorange3&quot;) lines(P2,z,lty=2,col=&quot;darkorange3&quot;) legend(&quot;topright&quot;,c(&quot;Stone 2000&quot;,&quot;ERA40&quot;),lty=c(1,2)) 2.1.2 Computation of scaling factors We can now compute the scaling factors according to Stone (2000). Same as above, to get some information about the function (parameters definition) type ?st_scaling in the R console. Code st = scaling_st(P1,lat) # here we use the pressure according to Stone 2000 model The result is stored in st as a dataframe with as many rows as there are elements in the input pressure vector (P1) and two columns named Nneutrons and Nmuons, for the spallogenic and muogenic contributions, respectively. Code st We can plot the evolution with elevation, which illustrates the major influence of altitude of the sampling site in controlling the local production rate. Code plot(st$Nneutrons,z,type=&quot;l&quot;, xlab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;,ylab=&quot;Altitude (m)&quot;, main=paste(&quot;Latitude &quot;,lat,&quot;°&quot;,sep=&quot;&quot;),col=&quot;darkorange3&quot;) Modify lat to see the effects on the scaling factor Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) st = scaling_st(P1,lat) # here we use the pressure according to Stone 2000 model plot(st$Nneutrons,z,type=&quot;l&quot;, xlab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;,ylab=&quot;Altitude (m)&quot;, main=paste(&quot;Latitude &quot;,lat,&quot;°&quot;,sep=&quot;&quot;),col=&quot;darkorange3&quot;) 2.1.3 Global variations In order to get a better idea of the variations with both latitude (from 0 to 90°) and elevation (from sea level to 3000 m) we can try the represent the evolution of the scaling factor for both parameters. Code P = atm_pressure(alt=0,model=&quot;stone2000&quot;) # compute pressure lat = seq(0,90,by=1) # latitude vector n = length(lat) # size of vector # st = scaling_st(P,lat) # compute scaling at sea level plot(lat,st$Nneutrons,type=&quot;l&quot;,ylim=c(0.5,12),col=&quot;darkorange3&quot;, xlab=&quot;Latitude (°)&quot;,ylab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;) grid() text(lat[n],st$Nneutrons[n],&quot;0 km&quot;,cex=0.5,adj=0) # put label at the end of curve # for (z in seq(500,3000,by=500)){ # loop on elevations : same as above for a range of elevations P = atm_pressure(alt=z,model=&quot;stone2000&quot;) st = scaling_st(P,lat) lines(lat,st$Nneutrons,col=&quot;darkorange3&quot;) text(lat[n],st$Nneutrons[n],z/1000,cex=0.5,adj=0) } This dependence of the scaling factor on latitude is a direct consequence of the dipole structure of the Earth magnetic field, with a higher cosmic rays flux at high latitudes. Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) P = atm_pressure(alt=0,model=&quot;stone2000&quot;) # compute pressure lat = seq(0,90,by=1) # latitude vector n = length(lat) # size of vector # st = scaling_st(P,lat) # compute scaling at sea level plot(lat,st$Nneutrons,type=&quot;l&quot;,ylim=c(0.5,12),col=&quot;darkorange3&quot;, xlab=&quot;Latitude (°)&quot;,ylab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;) grid() text(lat[n],st$Nneutrons[n],&quot;0 km&quot;,cex=0.5,adj=0) # put label at the end of curve # for (z in seq(500,3000,by=500)){ # loop on elevations : same as above for a range of elevations P = atm_pressure(alt=z,model=&quot;stone2000&quot;) st = scaling_st(P,lat) lines(lat,st$Nneutrons,col=&quot;darkorange3&quot;) text(lat[n],st$Nneutrons[n],z/1000,cex=0.5,adj=0) } 2.2 Time-dependent scalings 2.2.1 Definition of paleomagnetic variations Time-dependent scaling factors allow to take into account the variations through time of the Earth magnetic field, which modulates the incoming cosmic ray flux. This is particularly important in exposure dating applications. 2.2.1.1 Virtual Dipole Moment We need to first define a time series for the Virtual Dipole Moment (VDM) variation, using the get_vdm function. Several paleomagnetic databases can be used. The three options correspond to databases defined in Crep. We first extract the values of the Virtual Dipole Moment at specified times (vector time), and then plot the result. Code time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) Try to modify the time vector and change the model option to lsd or musch 2.2.1.2 Cutoff Rigidity Now we need to convert that into cutoff rigidity using vdm2rc function. Such can be done using the following expression (Martin et al. (2017)): \\[R_c = 14.3 \\frac{M}{M_0}\\cos^4 \\lambda,\\] where \\(M\\) is the moment of the Earth dipole field, \\(M_0\\) the 2010 reference value for \\(M\\) and \\(\\lambda\\) the latitude. This corresponds to the default model=\"elsasser54\" in the vdm2rc function arguments. A more complex formulation proposed by Lifton, Sato, and Dunai (2014) can be used with model=\"lifton14\". Code lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) Change the latitude lat and observe the influence on \\(R_c\\) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) 2.2.2 Lal/Stone modified scaling (lm) Once we have a \\(R_c\\) time series we can compute the lm scaling factors using the scaling_lm function. For that we will only use one elevation (z=0), so we recompute the atmospheric pressure. We plot the corresponding time series, as well as the value of st scaling factor for reference. Code P = atm_pressure(alt=0,model=&quot;stone2000&quot;) lm = scaling_lm(P,rc1) plot(time,lm,type=&quot;l&quot;,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Spallogenic lm scaling factor&quot;) abline(h=scaling_st(P,lat)$Nneutrons,lty=2) Explore the variations of the scaling factor by using various values for elevation alt, and different \\(R_c\\) time-series. Look for the differences with the time-independant st scaling. Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) P = atm_pressure(alt=0,model=&quot;stone2000&quot;) lm = scaling_lm(P,rc1) plot(time,lm,type=&quot;l&quot;,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Spallogenic lm scaling factor&quot;) abline(h=scaling_st(P,lat)$Nneutrons,lty=2) 2.3 Interactive shiny app You can also explore dynamically the behavior of scaling parameters using this embedded application, which use the same type of code. "],["exploring-tcn-build-up-at-the-surface.html", "3 Exploring TCN build up at the surface 3.1 Background 3.2 Set up of the calculations 3.3 Evolution of concentration with time 3.4 Two end-member situations 3.5 Comparing the Eulerian and Lagrangian descriptions 3.6 Interactive shiny app", " 3 Exploring TCN build up at the surface We are going to consider simple computations of concentration under various conditions in terms of erosion, depth or age. This will be done using an Eulerian point of view, which is the most straightforward and fastest way to perform such computation. In this case the quantity of interest (concentration) is computed at fixed depths below the surface, while the exhumed material is moving through this reference frame during its trajectory toward the surface. More details on the differences between Eulerian and Lagrangian approaches, and their applications to complex exposition/denudation histories, will be presented later. Note that interpreting measured concentrations in terms of end-member situations of pure exposure or steady-state denudation is often done with online calculators (Balco et al. (2008),Marrero et al. (2016),Martin et al. (2017)). Those calculators allow to perform very accurate computations of age or denudation rate, but one should always be careful about the underlying hypotheses (no erosion, steady state achieved, etc …) when interpreting measured concentrations and always think about how TCN are accumulating in the sampled material. The goal of this activity is to explore this behavior, using simple approaches. 3.1 Background The relevant general equation is the following, \\[\\begin{equation} C=C_0e^{-\\lambda t} + \\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda}e^{\\frac{-\\rho z}{\\Lambda_i}}(1-e^{-(\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda)t}) \\tag{3.1} \\end{equation}\\] with the following variables and parameters, \\(C\\) the concentration (as a function of time \\(t\\) and depth \\(z\\)) \\(C_0\\) the inherited concentration \\(\\lambda\\) the decay constant for the considered nuclide \\(P_i\\) the scaled surface production rate for the nuclide of interest and the \\(i\\)-th production pathway (spallation, stopped muons, fast muons) \\(\\rho\\) the density of the medium \\(\\Lambda_i\\) the attenuation length for the particules of the \\(i\\)-th production pathway \\(\\varepsilon\\) surface denudation In order to stick with usual conventions in the following time \\(t\\) will be measured in years (a), the unit of length will be cm and the depths (\\(z\\)) will be expressed in g/cm\\(^2\\) (i.e. actual depth \\(\\times \\rho\\)). Note two keys limitations of this representation : it does not allow to account for time variations of production rates (at least in its most straightforward implementation), so we will mostly using the st scaling it assumes exponential evolution of production with depth, which is clearly not the case for low energy neutrons (figure 2b from Gosse and Phillips (2001)) and is questionable in some situations for muons (Balco (2017)) 3.2 Set up of the calculations We should introduce some of the basic parameters we are going to use for the computation. For easy reference a set of data for parameters of interest is included in the TCNtools package. a vector (Lambda) with the attenuation lengths for different particles (in g/cm\\(^2\\)) neutrons for spallation reactions \\(\\Lambda_{spal}\\) stopping muons \\(\\Lambda_{stop}\\) fast muons \\(\\Lambda_{fast}\\) a vector (prm) with the SLHL production rates (in at/g/a), in this case for the st scaling scheme (Stone (2000)), and decay constant \\(\\lambda\\) (in 1/a) for the nuclides of interest. Note that this will need to be modified when using other scaling schemes. Note that these often used SLHL values are defined for convenience, most calculators for exposure age work directly with the production rate value at calibration sites, and that they are always relative to the scaling scheme used (Borchers et al. (2016)). We can first load the attenuation length data (g/cm\\(^2\\)). Documentation of this dataset is accessible by typing ?Lambda in the R console. Code data(Lambda) # we load a vector containing the attenuation length into the environment print(Lambda) ## Lspal Lstop Lfast ## 160 1500 4320 Code rho = 2.7 # we also define the density (g/cm3) Some production and decay parameters can also be loaded. Documentation of this dataset is accessible with ?prm. Code data(prm) # we load a matrix containing the production/decay parameters into the environment print(prm) ## Be10 Al26 C14 ## Pspal 4.01000e+00 2.793000e+01 1.224000e+01 ## Pstop 1.20000e-02 8.400000e-01 3.310000e+00 ## Pfast 3.90000e-02 8.100000e-02 0.000000e+00 ## lambda 5.09667e-07 9.667325e-07 1.209681e-04 We also need to define the properties of our site of interest and compute the relevant scaling parameters. As we already saw previously, this can easily be done with, Code altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) 3.3 Evolution of concentration with time To get a general overview of the behavior we are going to use directly the solv_conc_eul function, which allows to easily deal with various scenarios and configurations, using equation (3.1). In the following chapters we will go back to the key equations to get a better sense of the importance of various parameters. As always the documentation of the function, including its various arguments, can be obtained by typing ?solv_conc_eul in the R console. We start by defining the various Code nuc = &quot;Be10&quot; # &quot;Al26&quot;, &quot;C14&quot; t = seq(0,200e3,length.out=1000) # a vector containing time from 0 to 100 ka by 100 a steps z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration (at/g) ero = 0 * (100/1e6*rho) # denudation rate expressed in m/Ma and converted in g/cm2/a Now we can compute the concentration (at various times t), according to equation (3.1). Code C = solv_conc_eul(z,ero,t,C0,prm[,nuc],S,Lambda) # compute concentration Then we can plot the evolution of concentration with time. Code plot(t/1000,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # see explanation for these lines below Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions below) lambda = prm[4,nuc] # radiactive decay abline(0,sum(Prod)*1000,lty=2) # note that time is in ka on the plot abline(h=sum(Prod/((ero/Lambda)+lambda)),lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; # &quot;Al26&quot;, &quot;C14&quot; t = seq(0,200e3,length.out=1000) # a vector containing time from 0 to 100 ka by 100 a steps z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration (at/g) ero = 0 * (100/1e6*rho) # denudation rate expressed in m/Ma and converted in g/cm2/a C = solv_conc_eul(z,ero,t,C0,prm[,nuc],S,Lambda) # compute concentration plot(t/1000,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # see explanation for these lines below Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions below) lambda = prm[4,nuc] # radiactive decay abline(0,sum(Prod)*1000,lty=2) # note that time is in ka on the plot abline(h=sum(Prod/((ero/Lambda)+lambda)),lty=2) Starting with zero erosion (\\(\\varepsilon=0\\)), corresponding to the pure exposition of a surface, we see the progressive build-up of concentration though time and the establishment of a balance between gains (production) and losses (denudation and decay) leading to the concentration plateau at steady state. Note that two dashed lines are added to the graph, The first one corresponds to the production slope \\(\\sum_i P_i\\), how much nuclide you produce and how it would accumulate if you had no radioactive decay and no denudation. The second one (horizontal, not visible with the initial parameters) is the maximum value of concentration when steady state is achieved : \\[ C_{max}=\\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda} \\] Change the maximum of the t vector until you see the influence of radioactive decay and the plateau. Add some inheritance (\\(C_0\\)) Test the evolution with other nuclides such as \\(^{26}\\)Al and \\(^{14}\\)C (set nuc to Al26 or C14) Modify the ero parameter above (always keeping it is in g/cm\\(^2\\)/a), to see its influence on time needed to reach steady state and the final concentration 3.4 Two end-member situations Now we are going to build a summary plot showing the influence of both exposure and denudation. Code nuc = &quot;Be10&quot; # choice of nuclide t = 10^seq(log10(1),log10(10e6),length.out=1000) # time vector, log-spaced! # calculation of the evolution of concentration for denudation = 0 C = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure plot(t,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (a)&quot;,log=&quot;xy&quot;) grid() text(max(t),max(C),0,cex=0.5,adj=0) # label the curve # now we make the same computation for other denudation rates (using a loop) ero = c(1,10,100,1000) # erosion vector in m/Ma for (i in 1:length(ero)){ e = ero[i] * (100/1e6*rho) # convert denudation in g/cm2/a C = solv_conc_eul(0,e,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t,C,col=&quot;cornflowerblue&quot;,lwd=3) text(max(t),max(C),ero[i],cex=0.5,adj=0) # label the curve } Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; # choice of nuclide t = 10^seq(log10(1),log10(10e6),length.out=1000) # time vector, log-spaced! # calculation of the evolution of concentration for denudation = 0 C = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure plot(t,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (a)&quot;,log=&quot;xy&quot;) grid() text(max(t),max(C),0,cex=0.5,adj=0) # label the curve # now we make the same computation for other denudation rates (using a loop) ero = c(1,10,100,1000) # erosion vector in m/Ma for (i in 1:length(ero)){ e = ero[i] * (100/1e6*rho) # convert denudation in g/cm2/a C = solv_conc_eul(0,e,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t,C,col=&quot;cornflowerblue&quot;,lwd=3) text(max(t),max(C),ero[i],cex=0.5,adj=0) # label the curve } Note that this a log-log plot. It is probably one of the most important figure to keep in mind when analyzing TCN concentrations. It clearly shows the existence of two end-member situations when interpreting these concentrations, in terms of exposure age or denudation rates, and the transition between the two. Think about a bit about the following points What are the key hypotheses made when interpreting a TCN concentration in terms of exposure age surface denudation Can you think of geological/geomorphological situations where these hypotheses are invalid? Why are the plateau concentrations so different? How is the time to reach the plateau changing and why? 3.5 Comparing the Eulerian and Lagrangian descriptions This section presents a simple comparison of the Eulerian and Lagrangian approaches for the computation of cosmogenic nuclides concentration evolution in complex denudation and exposure scenarios. A nice presentation of this distinction is provided in Knudsen, Egholm, and Jansen (2019). 3.5.1 Definition of the denudation scenario We will start from a steady state situation under constant denudation, and then apply a change in the denudation rate for a given period of time. Code tmax = 2e6 # duration in a ero = 1/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a fact = 5 # change factor Here the duration of the simulation will be 2 Ma, the initial denudation rate will be 1 m/Ma, which will changed by a factor 5 at the beginning of the simulation. 3.5.2 Computation of concentrations with the Eulerian reference frame We compute the evolution of concentrations at the surface (\\(z=0\\)) using an Eulerian point of view. The calculation is very straightforward with to the solv_conc_eul function, note the use of in_ero parameter, to specify a starting denudation rate. Code df_e = data.frame(t = seq(0,tmax,length.out = 10000)) df_e$C10 = solv_conc_eul(0,ero*fact,df_e$t,0,prm[,&quot;Be10&quot;],S,Lambda,in_ero=ero) 3.5.3 Computation of concentrations with the Lagrangian reference frame Now we do the same calculation using a Lagrangian point of view, i.e. with a reference frame attached to a rock particle during its journey toward the surface as a response to denudation. It is slightly more complicated than in the Eulerian case and is done using the solv_conc_lag function. We first initiate a dataframe (df_l) and compute the evolution in depth z of the particle through time when it is exhumed from its initial depth (here 10 m) to the surface at a rate 5 m/Ma. Code df_l = data.frame(t=seq(0,tmax,length.out = 10000)) # data frame to store results df_l$z = ero*fact*df_l$t # cumulative erosion trough time (g/cm2) df_l$z = max(df_l$z) - df_l$z # convert into depth We compute the steady state concentration at starting depth, using the Eulerian solver. Code C10_0 = solv_conc_eul(max(df_l$z),ero,Inf,0,prm[,&#39;Be10&#39;],S,Lambda) # starting 10Be concentration We then define another column in the data frame with the evolution of scaling parameters through time. It is trivial here in the case of the time-independent st scaling, but it will be highly valuable when dealing with time-dependent scalings, where the Eulerian approach is not applicable. Code df_l$Ssp = rep(as.numeric(S[1]),nrow(df_l)) # scaling spallation df_l$Smu = rep(as.numeric(S[2]),nrow(df_l)) # scaling muons Then we calculate the un-scaled production rates at depths of interest. Here again, as we use simple models of exponential decrease for production with depth, the interest of using such Lagrangian approach is not obvious. But such way of computing concentration will allow to deal with non-exponential production profiles, which might necessary in some situation for muons or low-energy neutrons. Code # 10Be (not scaled) df_l$Psp10 = prm[&quot;Pspal&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lspal&quot;]) # spallation df_l$Pmu10 = prm[&quot;Pstop&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lstop&quot;]) + prm[&quot;Pfast&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lfast&quot;]) # muons We can now use the solv_conc_lag function to compute the evolution of concentration through time and depth. Code df_l$C10 = solv_conc_lag(df_l$t,df_l$z,C10_0,df_l$Psp10,df_l$Pmu10,prm[&quot;lambda&quot;,&#39;Be10&#39;],cbind(df_l$Ssp,df_l$Smu),final=FALSE) 3.5.4 Comparison of results We can now plot the results of the calculations and compare the two approaches. Code col_e = &quot;chartreuse&quot; col_l = &quot;chocolate1&quot; plot(df_e$t/1e6,df_e$C10/1e6,type=&quot;l&quot;,col=col_e,lwd=2, xlim=range(df_e$t/1e6),ylim=range(df_e$C10/1e6,df_l$C10/1e6), xlab=&quot;Time (Ma)&quot;,ylab=&quot;10Be concentration (x10e6 at/g)&quot;) grid() lines(df_l$t/1e6,df_l$C10/1e6,col=col_l,lwd=2) legend(&quot;topright&quot;,c(&quot;Eulerian&quot;,&quot;Lagrangian&quot;),lwd=2,col=c(col_e,col_l)) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) tmax = 2e6 # duration in a ero = 1/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a fact = 5 # change factor df_e = data.frame(t = seq(0,tmax,length.out = 10000)) df_e$C10 = solv_conc_eul(0,ero*fact,df_e$t,0,prm[,&quot;Be10&quot;],S,Lambda,in_ero=ero) df_l = data.frame(t=seq(0,tmax,length.out = 10000)) # data frame to store results df_l$z = ero*fact*df_l$t # cumulative erosion trough time (g/cm2) df_l$z = max(df_l$z) - df_l$z # convert into depth C10_0 = solv_conc_eul(max(df_l$z),ero,Inf,0,prm[,&#39;Be10&#39;],S,Lambda) # starting 10Be concentration df_l$Ssp = rep(as.numeric(S[1]),nrow(df_l)) # scaling spallation df_l$Smu = rep(as.numeric(S[2]),nrow(df_l)) # scaling muons # 10Be (not scaled) df_l$Psp10 = prm[&quot;Pspal&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lspal&quot;]) # spallation df_l$Pmu10 = prm[&quot;Pstop&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lstop&quot;]) + prm[&quot;Pfast&quot;,&#39;Be10&#39;]*exp(-1*df_l$z/Lambda[&quot;Lfast&quot;]) # muons df_l$C10 = solv_conc_lag(df_l$t,df_l$z,C10_0,df_l$Psp10,df_l$Pmu10,prm[&quot;lambda&quot;,&#39;Be10&#39;],cbind(df_l$Ssp,df_l$Smu),final=FALSE) col_e = &quot;chartreuse&quot; col_l = &quot;chocolate1&quot; plot(df_e$t/1e6,df_e$C10/1e6,type=&quot;l&quot;,col=col_e,lwd=2, xlim=range(df_e$t/1e6),ylim=range(df_e$C10/1e6,df_l$C10/1e6), xlab=&quot;Time (Ma)&quot;,ylab=&quot;10Be concentration (x10e6 at/g)&quot;) grid() lines(df_l$t/1e6,df_l$C10/1e6,col=col_l,lwd=2) legend(&quot;topright&quot;,c(&quot;Eulerian&quot;,&quot;Lagrangian&quot;),lwd=2,col=c(col_e,col_l)) The Lagrangian evolution starts with a low concentration corresponding to the burial at depth of the particle at the onset of the evolution, while at the same time (\\(t=0\\)) we are looking at the surface with the Eulerian point of view, and then obviously observe much higher concentrations. The two descriptions converge toward the same concentration at the end of the evolution (i.e. when the lagrangian particle reaches the surface). Try to think about various contexts and applications where the use of the Eulerian or Lagrangian appraoch could be preferred, and why. 3.6 Interactive shiny app You can also explore dynamically the TCN build up using this embedded application, which use the same type of code. "],["application-to-exposure-dating.html", "4 Application to exposure dating 4.1 Back to the evolution of concentration 4.2 Interpretation in terms of exposure ages 4.3 Time varying production rates 4.4 Dealing with uncertainties", " 4 Application to exposure dating 4.1 Back to the evolution of concentration We first consider the evolution of concentration with time \\(t\\). The computation will be carried out at the surface (\\(z=0\\)), but this could be done at any arbitrary depth. We also consider that \\(\\varepsilon = 0\\) and that there is no inheritance. The starting equation above becomes, \\[\\begin{equation} C(t)=\\sum_i \\frac{P_i}{\\lambda}(1-e^{-\\lambda t}) \\tag{4.1} \\end{equation}\\] For the sake of the example we are going to compute that by hand, and compare with the results obtained with the solv_conc_eul function, Again we define the usual parameters Code # set up altitude = 1000 # elevation in m latitude = 45 # latitude in degrees nuc = &quot;Be10&quot; P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) To improve readability we compute scaled production rates and give them explicit names. Code Pspal = prm[1,nuc]*S$Nneutrons # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,nuc]*S$Nmuons # scaled stopped muons production rate in at/g/y Pfast = prm[3,nuc]*S$Nmuons # scaled fast muons production rate in at/g/y lambda = prm[4,nuc] # radioactive decay (1/y) And then apply equation (4.1) and compare the results with the output of function solv_conc_eul. Code t = seq(0,200e3,by=100) # a vector containing time from 0 to 100 ka by 100 a steps C = (Pspal + Pstop + Pfast) / lambda * (1-exp(-lambda*t)) plot(t/1000,C,type=&quot;l&quot;,col=&quot;coral&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # check if this is ok C2 = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t/1000,C2,lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # # set up altitude = 1000 # elevation in m latitude = 45 # latitude in degrees nuc = &quot;Be10&quot; P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Pspal = prm[1,nuc]*S$Nneutrons # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,nuc]*S$Nmuons # scaled stopped muons production rate in at/g/y Pfast = prm[3,nuc]*S$Nmuons # scaled fast muons production rate in at/g/y lambda = prm[4,nuc] # radioactive decay (1/y) t = seq(0,200e3,by=100) # a vector containing time from 0 to 100 ka by 100 a steps C = (Pspal + Pstop + Pfast) / lambda * (1-exp(-lambda*t)) plot(t/1000,C,type=&quot;l&quot;,col=&quot;coral&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # check if this is ok C2 = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t/1000,C2,lty=2) One thing to note in this graph is that time \\(t\\) is expressed as time elapsed since start of exposure, which we can not directly translate into the age BP, when dealing with radioactive decay and time variations in production rate. Change the maximum time t to check that everything is correct. 4.2 Interpretation in terms of exposure ages We are now going to make some simple calculations to interpret concentrations in terms of exposure age. The objective is solving for \\(t\\) the equation \\(C_{mod}(t) = C_{mes}\\). The left hand side is our modelled concentration, obtained for example with equation (3.1) (solv_conc_eul), where we need to keep in mind all the hypotheses we make (\\(z=0\\), \\(\\varepsilon=0\\), \\(C_0=0\\)). We are going to use an example from Protin et al. (2019), using sample ARG-16-9, which was collected from a lateral moraine of the Argentière glacier (Northern French Alps). Here are the characteristics of this sample (see table 1 from this paper). Code rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Now, for example we can make some guess about the exposure age and see what is the difference with the measured concentration. Code age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration Below you can get all the necessary code to compute Cmod Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration Change the value of age and try to get close to measured \\(^{10}\\)Be concentration. We could try make Cmod equal to Cmes by trial and error, but we can be more systematic. We go through automatically over a whole range of age values, and compute the corresponding Cmod values. Code age = seq(0,20e3,by=10) # we define a vector for age, which set the different explored values Cmod = rep(NA,length(age)) # The vector of same length to store the results for (i in 1:length(age)){ Cmod[i] = solv_conc_eul(0,0,age[i],0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration } We then plot the results to identify the minimum value of \\(|C_{mod} - C_{mes}|\\) Code imin = which.min(abs(Cmod-Cmes)) # what is the index of the minimum difference res = age[imin] # the corresponding age plot(age,abs(Cmod-Cmes),type=&quot;l&quot;, xlab=&quot;Age (a)&quot;, main=paste(res,&quot;a BP&quot;),col=&quot;coral&quot;,lwd=2) abline(h=0) abline(v=res) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration age = seq(0,20e3,by=10) # we define a vector for age, which set the different explored values Cmod = rep(NA,length(age)) # The vector of same length to store the results for (i in 1:length(age)){ Cmod[i] = solv_conc_eul(0,0,age[i],0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration } imin = which.min(abs(Cmod-Cmes)) # what is the index of the minimum difference res = age[imin] # the corresponding age plot(age,abs(Cmod-Cmes),type=&quot;l&quot;, xlab=&quot;Age (a)&quot;, main=paste(res,&quot;a BP&quot;),col=&quot;coral&quot;,lwd=2) abline(h=0) abline(v=res) Compare with the age reported in table 1 from Protin et al. (2019). Comment about the possible causes for the differences? We can be even more efficient by using an simple optimization approach to solve \\(C_{mod}(t) = C_{mes}\\), using the built in optimize function. We just define a function to be optimized (search of minimum) and launch a search over an age range. We are looking for the minimum value of \\(|C_{mod}(t)-C_{mes}|\\). Here the function we are trying to optimize. Code fun_opt &lt;-function(t,Cmes,prm,S,Lambda){ Cmod = solv_conc_eul(0,0,t,0,prm[,&quot;Be10&quot;],S,Lambda) return(abs(Cmod-Cmes)) } We then launch the search of the 0-50 ka interval. Code res = optimize(fun_opt,c(0,50e3),Cmes,prm,S*Ss,Lambda) print(res) ## $minimum ## [1] 12478.57 ## ## $objective ## [1] 0.0008513347 Below you can get all the necessary code to obtain this result Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) fun_opt &lt;-function(t,Cmes,prm,S,Lambda){ Cmod = solv_conc_eul(0,0,t,0,prm[,&quot;Be10&quot;],S,Lambda) return(abs(Cmod-Cmes)) } res = optimize(fun_opt,c(0,50e3),Cmes,prm,S*Ss,Lambda) print(res) This is a very coarse calculation, but searching the solution for \\(C_{mod}(t)=C_{mes}\\) is the essence of what online calculator do. What kind of improvement could you think of concerning this calculation? 4.3 Time varying production rates We are now going back to the time-varying scaling schemes, such as the one we explored in the previous chapter. Code data = data.frame(t1=seq(0,20e3,length.out=2000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,latitude,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc) plot(data$t2,data$lm,type=&quot;l&quot;,xlab=&quot;Age BP (a)&quot;,ylab=&quot;Scaling factor&quot;) abline(h=S$Nneutrons,col=&quot;red&quot;) abline(h=mean(data$lm),lty=2) In this case we can not use the same equations for the evolution of TCN concentration as above, because we need to account for \\(P(t)\\). We are going to use, \\[C_{mod}(t) = e^{-\\lambda t} \\int_0^t P(t&#39;)e^{\\lambda t&#39;}dt&#39;\\] Important notes : It is usually considered that production by muons is much less affected than that by neutrons by the magnetic field variations. You might notice that the equation above is different than, for example, equation 1 from Balco et al. (2008), of the form \\(C_{mod}(T) = \\int_0^T P(t&#39;)e^{-\\lambda t&#39;}dt&#39;\\). This just because the latter refers to exposure age BP, whereas we are thinking in terms of time since exposure here. Below we first compare how the concentration evolves through time with the two approaches. Code library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # Pspal = prm[1,&quot;Be10&quot;]*S$Nneutrons*Ss # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,&quot;Be10&quot;]*S$Nmuons*Ss # scaled stopped muons production rate in at/g/y Pfast = prm[3,&quot;Be10&quot;]*S$Nmuons*Ss # scaled fast muons production rate in at/g/y lambda = prm[4,&quot;Be10&quot;] # radioactive decay (1/y) data$Prod_st = Pspal + Pstop + Pfast data$C1 = solv_conc_eul(0,0,data$t1,0,prm,S*Ss,Lambda) # data$Prod_lm = prm[1,&quot;Be10&quot;]*data$lm*Ss+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*Ss*S$Nmuons data$C2 = exp(-lambda*data$t1)*cumtrapz(data$t1,data$Prod_lm*exp(lambda*data$t1)) # plot(data$t1,data$C1,type=&quot;l&quot;,xlab=&quot;Time since start of exposure (a)&quot;,ylab=&quot;Concentration (at/g)&quot;,col=&quot;cyan4&quot;) lines(data$t1,data$C2,col=&quot;darkgoldenrod2&quot;) legend(&quot;topleft&quot;,c(&quot;st&quot;,&quot;lm&quot;),lty=1,col=c(&quot;cyan4&quot;, &quot;darkgoldenrod2&quot;)) Comment on the differences. We can solve solve \\(C_{mod}(t)=C_{mes}\\) for \\(t\\) with the same approach as the st case, Code library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # compute_C&lt;-function(t1,prm,P,lat,Lambda,S,Ss){ data = data.frame(t1=seq(0,t1,length.out=1000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,lat,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc)*Ss Cmod = exp(-prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1[nrow(data)])*trapz(data$t1,(prm[1,&quot;Be10&quot;]*data$lm+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*S$Nmuons)*(exp(prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1))) return(Cmod) } # fun_opt2 &lt;-function(t1,Cmes,prm,P,lat,Lambda,S,Ss){ Cmod = compute_C(t1,prm,P,lat,Lambda,S,Ss) return(abs(Cmod-Cmes)) } # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) res = optimize(fun_opt2,c(0,50e3),Cmes,prm,P,latitude,Lambda,S,Ss) print(res) ## $minimum ## [1] 12562.66 ## ## $objective ## [1] 0.001908477 Below you can get all the necessary code to obtain this result Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # compute_C&lt;-function(t1,prm,P,lat,Lambda,S,Ss){ data = data.frame(t1=seq(0,t1,length.out=1000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,lat,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc)*Ss Cmod = exp(-prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1[nrow(data)])*trapz(data$t1,(prm[1,&quot;Be10&quot;]*data$lm+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*S$Nmuons)*(exp(prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1))) return(Cmod) } # fun_opt2 &lt;-function(t1,Cmes,prm,P,lat,Lambda,S,Ss){ Cmod = compute_C(t1,prm,P,lat,Lambda,S,Ss) return(abs(Cmod-Cmes)) } # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) res = optimize(fun_opt2,c(0,50e3),Cmes,prm,P,latitude,Lambda,S,Ss) print(res) Caveat we did those calculation using a number of simplification and numerical shortcuts …. 4.4 Dealing with uncertainties "],["application-to-denudation-rate-measurements.html", "5 Application to denudation rate measurements 5.1 Concentration - denudation rate relationship 5.2 Calculation of denudation rates 5.3 Integration time scale 5.4 Catchment wide denudation rates 5.5 Change in denudation through time 5.6 On the importance of muons", " 5 Application to denudation rate measurements 5.1 Concentration - denudation rate relationship Now we are going to consider the evolution of concentration with denudation rate \\(\\varepsilon\\). The computation will be carried out at the surface (\\(z=0\\)), but this could be done at any arbitrary depth. We will consider that \\(t=+\\infty\\) and that we have reached the plateau concentration. Equation (3.1) becomes, \\[\\begin{equation} C=\\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda} \\tag{5.1} \\end{equation}\\] Which simplifies to \\(C\\approx\\frac{P_i \\Lambda_i}{\\rho \\varepsilon}\\) if we neglect radioactive decay. We define the usual parameters. Code altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Now compute the steady-state concentration for a range of denudation rates. Code nuc = &quot;Be10&quot; ero = 10^seq(log10(0.1),log10(1000),length.out = 100) * 100/1e6*rho # a log-spaced vector for denudation rate expressed in m/Ma and converted in g/cm2/a age = Inf # infinite age z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration C = solv_conc_eul(z,ero,age,C0,prm[,nuc],S,Lambda) # compute concentration We plot the result, and also consider consider the implications of neglecting the radioactive decay. Code plot(ero/100*1e6/rho,C,col=&quot;lawngreen&quot;,log=&quot;xy&quot;,type=&quot;l&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) # what happens if we neglect radioactive decay Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions) lambda = prm[4,nuc] # radioactive decay C2 = sum(Prod*Lambda)/ero lines(ero/100*1e6/rho,C2,lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; ero = 10^seq(log10(0.1),log10(1000),length.out = 100) * 100/1e6*rho # a log-spaced vector for denudation rate expressed in m/Ma and converted in g/cm2/a age = Inf # infinite age z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration C = solv_conc_eul(z,ero,age,C0,prm[,nuc],S,Lambda) # compute concentration plot(ero/100*1e6/rho,C,col=&quot;lawngreen&quot;,log=&quot;xy&quot;,type=&quot;l&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) # what happens if we neglect radioactive decay Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions) lambda = prm[4,nuc] # radioactive decay C2 = sum(Prod*Lambda)/ero lines(ero/100*1e6/rho,C2,lty=2) This figure (log-scales on both axes) highlights the strong inverse relationship, at steady-state, between denudation rate (\\(\\varepsilon\\)) and concentration (\\(C\\)), which is the foundation of many geomorphological studies trying to establish landscape evolution rates. Note the change in the relationship at very low denudation rates, which corresponds to the situation where the effects of radioactive decay become predominant. Over what range of denudation rates it is reasonable to neglect radioactive decay? What kind of geological context could it correspond to? A simple way to answer this question would be to compute the relative difference between the computed concentrations Code error = abs(C-C2)/C*100 plot(ero/100*1e6/rho,error,log=&quot;xy&quot;,type=&quot;l&quot;,ylab=&quot;Relative error (%)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) 5.2 Calculation of denudation rates We are going to load a subset of the octopus database from Codilean et al. (2022), which compile most of the published catchment-wide denudation rates and can be accessed through this web site : https://octopusdata.org The original database have been filtered to retain catchments with the following characteristics Area in the 20-100 km\\(^2\\) range Denudation rate \\(&lt;\\) 2 mm/a Code octopus = read.table(&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/octopus.dat&quot;,header=T) The data base contains a lot of information about the catchments. Code octopus We can have look at the distribution of these denudation rates. Code hist(octopus$EBE_MMKYR,breaks=50,xlab=&quot;Denudation rate (mm/ka)&quot;,main=&quot;&quot;) We can first make a very crude calculation, considering only production by neutrons and neglecting radioactive decay, using the following relationship, \\[\\varepsilon=\\frac{P \\Lambda}{\\rho C}\\] We load the usual parameters and make some definitions Code data(Lambda) data(prm) rho = 2.65 # same density in g/cm3 as the one used in Octopus We compute the scaling factors for each basin, using its average elevation to compute average scaling factors. This is a very important point and a section will be devoted to the analysis of this approximation. Code octopus$P = atm_pressure(alt=octopus$ELEV_AVE,model=&quot;stone2000&quot;) S = scaling_st(octopus$P,octopus$Y_WGS84) octopus = cbind(octopus,S) For the sake of readability we define a number of variables. Code Pspal = prm[&quot;Pspal&quot;,&quot;Be10&quot;]*octopus$Nneutrons # scaled spallation production rate Pstop = prm[&quot;Pstop&quot;,&quot;Be10&quot;]*octopus$Nmuons # scalled stopped muons production rate Pfast = prm[&quot;Pfast&quot;,&quot;Be10&quot;]*octopus$Nmuons # scalled fast muons production rate lambda = prm[&quot;lambda&quot;,&quot;Be10&quot;] C = octopus$BE10NC We can now compute denudation rates and compare them with the rates calculated in Octopus. Code octopus$E1 = (Pspal*Lambda[1])/(rho*C)*10*1000 # denudation rate in mm/ka plot(octopus$EBE_MMKYR,octopus$E1,pch=21,bg=&quot;lightblue&quot;,cex=0.5, xlab=&quot;Octopus denudation rate (mm/ka)&quot;,ylab=&quot;Recalculated denudation rate (mm/ka)&quot;) abline(0,1,col=&quot;red&quot;,lty=2) What can you say about this comparison? We compute an estimator of the difference for latter reference. Code ER1 = mean(((octopus$EBE_MMKYR-octopus$E1)/octopus$EBE_MMKYR)^2) ER1 ## [1] 0.01740892 Now we include muons in the calculation (still neglecting radioactive decay): \\[ \\varepsilon=\\sum_i \\frac{P_i\\Lambda_i}{\\rho C}\\] Code octopus$E2 = (Pspal*Lambda[1] + Pstop*Lambda[2] + Pfast*Lambda[3])/(rho*C)*10*1000 plot(octopus$EBE_MMKYR,octopus$E2,pch=21,bg=&quot;lightblue&quot;,cex=0.5, xlab=&quot;Octopus denudation rate (mm/ka)&quot;,ylab=&quot;Recalculated denudation rate (mm/ka)&quot;) abline(0,1,col=&quot;red&quot;,lty=2) Code ER2 = mean(((octopus$EBE_MMKYR-octopus$E2)/octopus$EBE_MMKYR)^2) ER2 ## [1] 0.006887197 Comment on the differences between the two calculations. The denudation rates span several orders of magnitude, so we use a log-log plot. Code plot(octopus$EBE_MMKYR,octopus$E2,log=&quot;xy&quot;,pch=21,bg=&quot;lightblue&quot;,cex=0.5) abline(0,1,col=&quot;red&quot;,lty=2) What happens for low denudation rates? Now we want to take into account radioactive decay, and have to deal with equation (5.1), which does not allow to compute denudation rate \\(\\varepsilon\\) directly. So we have to solve \\(C_{mod}(\\varepsilon) = C_{obs}\\). For that purpose we define a function to compute concentration for any value of \\(\\varepsilon\\), and a function to optimize the difference between the modeled and observed concentration : \\(|C_{mod}(\\varepsilon) - C_{obs}|\\). Code compute_C &lt;- function(ero,prm,S,Lambda){ C = solv_conc_eul(0,ero,Inf,0,prm,S,Lambda) # compute concentration return(C) } fun_opt &lt;-function(ero,prm,S,Lambda,Cmes){ Cmod = compute_C(ero,prm,S,Lambda) return(abs(Cmod-Cmes)) } Then we run the optimization for each basin. Code octopus$E3 = NA for (i in 1:nrow(octopus)){ res = optimize(fun_opt,c(0,3000)/10*rho,prm[,&quot;Be10&quot;],c(octopus$Nneutrons[i],octopus$Nmuons[i]), Lambda,octopus$BE10NC[i]) octopus$E3[i] = res$minimum*10/rho*1000 } And we compare again the denudation rates. Code plot(octopus$EBE_MMKYR,octopus$E3,log=&quot;xy&quot;,pch=21,bg=&quot;lightblue&quot;,cex=0.5) abline(0,1,col=&quot;red&quot;,lty=2) Code ER3 = mean(((octopus$EBE_MMKYR-octopus$E3)/octopus$EBE_MMKYR)^2) ER3 ## [1] 0.002865613 Comment this result. Below you can get all the necessary code to produce these figures Code octopus = read.table(&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/octopus.dat&quot;,header=T) hist(octopus$EBE_MMKYR,breaks=50,xlab=&quot;Denudation rate (mm/ka)&quot;,main=&quot;&quot;) data(Lambda) data(prm) rho = 2.65 # same density in g/cm3 as the one used in Octopus octopus$P = atm_pressure(alt=octopus$ELEV_AVE,model=&quot;stone2000&quot;) S = scaling_st(octopus$P,octopus$Y_WGS84) octopus = cbind(octopus,S) Pspal = prm[&quot;Pspal&quot;,&quot;Be10&quot;]*octopus$Nneutrons # scaled spallation production rate Pstop = prm[&quot;Pstop&quot;,&quot;Be10&quot;]*octopus$Nmuons # scalled stopped muons production rate Pfast = prm[&quot;Pfast&quot;,&quot;Be10&quot;]*octopus$Nmuons # scalled fast muons production rate lambda = prm[&quot;lambda&quot;,&quot;Be10&quot;] C = octopus$BE10NC octopus$E1 = (Pspal*Lambda[1])/(rho*C)*10*1000 # denudation rate in mm/ka plot(octopus$EBE_MMKYR,octopus$E1,pch=21,bg=&quot;lightblue&quot;,cex=0.5, xlab=&quot;Octopus denudation rate (mm/ka)&quot;,ylab=&quot;Recalculated denudation rate (mm/ka)&quot;) abline(0,1,col=&quot;red&quot;,lty=2) ER1 = mean(((octopus$EBE_MMKYR-octopus$E1)/octopus$EBE_MMKYR)^2) ER1 octopus$E2 = (Pspal*Lambda[1] + Pstop*Lambda[2] + Pfast*Lambda[3])/(rho*C)*10*1000 plot(octopus$EBE_MMKYR,octopus$E2,pch=21,bg=&quot;lightblue&quot;,cex=0.5, xlab=&quot;Octopus denudation rate (mm/ka)&quot;,ylab=&quot;Recalculated denudation rate (mm/ka)&quot;) abline(0,1,col=&quot;red&quot;,lty=2) ER2 = mean(((octopus$EBE_MMKYR-octopus$E2)/octopus$EBE_MMKYR)^2) ER2 plot(octopus$EBE_MMKYR,octopus$E2,log=&quot;xy&quot;,pch=21,bg=&quot;lightblue&quot;,cex=0.5) abline(0,1,col=&quot;red&quot;,lty=2) compute_C &lt;- function(ero,prm,S,Lambda){ C = solv_conc_eul(0,ero,Inf,0,prm,S,Lambda) # compute concentration return(C) } fun_opt &lt;-function(ero,prm,S,Lambda,Cmes){ Cmod = compute_C(ero,prm,S,Lambda) return(abs(Cmod-Cmes)) } octopus$E3 = NA for (i in 1:nrow(octopus)){ res = optimize(fun_opt,c(0,3000)/10*rho,prm[,&quot;Be10&quot;],c(octopus$Nneutrons[i],octopus$Nmuons[i]), Lambda,octopus$BE10NC[i]) octopus$E3[i] = res$minimum*10/rho*1000 } plot(octopus$EBE_MMKYR,octopus$E3,log=&quot;xy&quot;,pch=21,bg=&quot;lightblue&quot;,cex=0.5) abline(0,1,col=&quot;red&quot;,lty=2) ER3 = mean(((octopus$EBE_MMKYR-octopus$E3)/octopus$EBE_MMKYR)^2) ER3 5.3 Integration time scale Like all techniques used to measure erosion or denudation of the Earth surface, rates measured from cosmogenic nuclides are averaged over a specific period of time. This integration time-scale is often considered, at first order, to be the time required to erode the thickness of the zone where most of nuclide production occurs (von Blanckenburg (2005)), which can be considered as the penetration length for neutrons \\(\\Lambda/\\rho\\). This average is one of the advantages of the method, allowing to filter the high frequency variability, in particular of anthropic origin (Reusser, Bierman, and Rood (2015)). In its simplest form the integration time scale \\(\\tau\\) for a denudation rate measurement can be calculated as, \\[\\begin{equation} \\tau = \\frac{\\Lambda}{\\rho \\varepsilon} \\end{equation}\\] Code rho = 2.7 data(Lambda) ero = 10^seq(log10(0.5),log10(2000),length.out = 100) # log spaced vector for denudation rate mm/ka tau = (Lambda[1]/rho*10)/ero plot(ero,tau,xlab=&quot;Denudation rate (mm/ka)&quot;,ylab=&quot;Integration time scale (ka)&quot;,type=&quot;l&quot;,log=&quot;xy&quot;,lwd=3,col=&quot;gold&quot;) This simple plot allows to observe the very strong influence of denudation rate \\(\\varepsilon\\) on the integration time scale. For a denudation rate of 10 mm/ka, typical of slow erosion cratonic domains, the integration time scale is ~60 ka For a denudation rate of 500 mm/ka, which can be encountered in high relief mountain ranges, the integration time scale is ~1 ka What are the implications of this relationship for the geomorphological interpretation of the denudation rates? 5.4 Catchment wide denudation rates It can be noted that when dealing with eroding bedrock surfaces the calculated denudation rate will be a very local estimate and that its extrapolation to longer wavelengths is only possible in the rather limited cases where the morphological properties of the relief are uniform over large distances (plateaus, morphological surfaces, etc …). In reality, in complex environments, such as mountain ranges, where several processes contribute to denudation, a simple local estimate does not provide really useful information on the overall dynamics of the topography. An alternative approach is to use river sediments, which are a mixture of contributions from different parts of a watershed. The measurement of a cosmogenic nuclide concentration in this type of sample provides an estimate of the average denudation over the corresponding basin (Brown et al. (1995),Granger, Kirchner, and Finkel (1996),von Blanckenburg (2005)). Calculating such average denudation rate requires to obtain an average production rate for the catchment. For low relief catchments the scaling parameters can be calculated using the average elevation of the catchment. But for catchments with several 1000s m in relief, the non linear relationship between the scaling parameters and elevation will make this approximation invalid. We are going to explore these ideas using 3 basins from the study of Godard et al. (2012) in the Marsyandi river basin (central Nepal) Here are the maps for these 3 basins. We are going to load the corresponding data as dataframes (x, y, z), either from the disk or directly from the github repository (path_to_data indicates where to look for the data). Code #path_to_data = &quot;data/gis/&quot; path_to_data =&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/&quot; # we download directly from the github repository names_basins = c(&quot;Marsyandi&quot;,&quot;Chudi&quot;,&quot;Khudi&quot;) cols = c(&quot;deepskyblue&quot;, &quot;khaki3&quot;, &quot;darkolivegreen3&quot;) basins = list() # we store the dataframe into a list object for (i in 1:length(names_basins)){ basins[[i]] = read.table(paste0(path_to_data,names_basins[i],&quot;.dat&quot;),header=T) } We now plot the distribution of elevation for the 3 basins. Code # for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$z),ylim=c(0,0.0022),xlab=&quot;Elevation (m)&quot;,col=cols[i],main=&quot;&quot;) }else{ # then add the following basins as lines on the initial plot lines(density(basins[[i]]$z),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) Compare the distributions of elevation, and their position across the Himalayan range. How is this going to impact the determination of an average production rate for the basin? We then compute the st scaling parameters for each pixel (lines in the dataframes). Code for (i in 1:length(names_basins)){ basins[[i]]$P = atm_pressure(alt=basins[[i]]$z,model=&quot;stone2000&quot;) st = scaling_st(basins[[i]]$P,basins[[i]]$y) basins[[i]] = cbind(basins[[i]],st) } We plot the distribution of the spallation scaling factor for each basin. Code # plotting result for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$Nneutrons),ylim=c(1e-4,2.5),xlab=&quot;Spallation scaling factor&quot;,col=cols[i],main=&quot;&quot;,log=&quot;y&quot;) }else{ lines(density(basins[[i]]$Nneutrons),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) We now compute average values for the scaling paramters, in 2 different ways, 1 : We compute the scaling at each pixel according to its latitude and altitude (as we did above), and then take the average over the basin 2 : we compute the average latitude and altitude of the basin, and then use these average values to compute the scaling factors We create a dataframe res to store the results. Code res = data.frame(names=names_basins) # dataframe to store the results res$S1 = NA res$S2 = NA res$relief = NA for (i in 1:nrow(res)){ res$relief[i] = max(basins[[i]]$z) - min(basins[[i]]$z) # res$S1[i] = mean(basins[[i]]$Nneutrons) # P2 = atm_pressure(alt=mean(basins[[i]]$z),model=&quot;stone2000&quot;) st = scaling_st(P2,mean(basins[[i]]$y)) res$S2[i] = st$Nneutrons } print(res) ## names S1 S2 relief ## 1 Marsyandi 12.663441 9.444905 7676.423 ## 2 Chudi 1.420476 1.403903 1220.326 ## 3 Khudi 5.841666 5.168450 4065.374 We can visualize the difference between the two approaches. Code plot(res$S2,res$S1,pch=21,bg=cols, xlab=&quot;S2 : Scaling from average elevation&quot;,ylab=&quot;S1 : Scaling pixel by pixel&quot;, xlim=range(0,res$S2,res$S1),ylim=range(0,res$S2,res$S1)) grid() text(res$S2,res$S1,res$names,cex=0.5,pos=1) abline(0,1,lty=2) Below you can get all the necessary code to produce the figures presented in this section Code library(&quot;TCNtools&quot;) #path_to_data = &quot;data/gis/&quot; path_to_data =&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/&quot; # we download directly from the github repository names_basins = c(&quot;Marsyandi&quot;,&quot;Chudi&quot;,&quot;Khudi&quot;) cols = c(&quot;deepskyblue&quot;, &quot;khaki3&quot;, &quot;darkolivegreen3&quot;) basins = list() # we store the dataframe into a list object for (i in 1:length(names_basins)){ basins[[i]] = read.table(paste0(path_to_data,names_basins[i],&quot;.dat&quot;),header=T) } # for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$z),ylim=c(0,0.0022),xlab=&quot;Elevation (m)&quot;,col=cols[i],main=&quot;&quot;) }else{ # then add the following basins as lines on the initial plot lines(density(basins[[i]]$z),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) for (i in 1:length(names_basins)){ basins[[i]]$P = atm_pressure(alt=basins[[i]]$z,model=&quot;stone2000&quot;) st = scaling_st(basins[[i]]$P,basins[[i]]$y) basins[[i]] = cbind(basins[[i]],st) } # plotting result for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$Nneutrons),ylim=c(1e-4,2.5),xlab=&quot;Spallation scaling factor&quot;,col=cols[i],main=&quot;&quot;,log=&quot;y&quot;) }else{ lines(density(basins[[i]]$Nneutrons),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) res = data.frame(names=names_basins) # dataframe to store the results res$S1 = NA res$S2 = NA res$relief = NA for (i in 1:nrow(res)){ res$relief[i] = max(basins[[i]]$z) - min(basins[[i]]$z) # res$S1[i] = mean(basins[[i]]$Nneutrons) # P2 = atm_pressure(alt=mean(basins[[i]]$z),model=&quot;stone2000&quot;) st = scaling_st(P2,mean(basins[[i]]$y)) res$S2[i] = st$Nneutrons } print(res) plot(res$S2,res$S1,pch=21,bg=cols, xlab=&quot;S2 : Scaling from average elevation&quot;,ylab=&quot;S1 : Scaling pixel by pixel&quot;, xlim=range(0,res$S2,res$S1),ylim=range(0,res$S2,res$S1)) grid() text(res$S2,res$S1,res$names,cex=0.5,pos=1) abline(0,1,lty=2) Comment on the differences. What are the other parameters which can influence the calculation of average production rates at the scale of a basin? 5.5 Change in denudation through time The solv_conc_eul function has a in_ero parameter which allows to specify an initial denudation rate, and then simulate the evolution concentration as a response to a change in the denudation rate. Code altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 2.7 # nuc = &quot;Be10&quot; E1 = 10/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a E2 = 20/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a t = seq(0,1e6,length.out = 10000) C = solv_conc_eul(0,E2,t,0,prm[,nuc],S,Lambda,in_ero=E1) plot(t/1e6,C,xlab=&quot;Time (Ma)&quot;,ylab=&quot;Concentration (at/g)&quot;,type=&quot;l&quot;,col=&quot;gold&quot;,lwd=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 2.7 # nuc = &quot;Be10&quot; E1 = 10/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a E2 = 20/1.e6*100*rho # denudation rate conversion en m/Ma -&gt; g/cm2/a t = seq(0,1e6,length.out = 10000) C = solv_conc_eul(0,E2,t,0,prm[,nuc],S,Lambda,in_ero=E1) plot(t/1e6,C,xlab=&quot;Time (Ma)&quot;,ylab=&quot;Concentration (at/g)&quot;,type=&quot;l&quot;,col=&quot;gold&quot;,lwd=2) Change the values of the denudation rates E1 and E2 Change the nuclide (use “Al26” and “C14”) What are the implications of changes in denudation rates for the interpretation of TCN concentrations? What kind of geomorphological situation could it correspond to? Further reading for the \\(^{14}\\)C/\\(^{10}\\)Be system in Mudd (2016). What kinf of information does the use of multiple nuclides can provide? You can also explore dynamically the impact of changes in denudation rates using this embedded application, which use the same type of code. 5.6 On the importance of muons For \\(^{10}\\)Be the muonic contribution represents a very small fraction of surface production : Code nuc = &quot;Be10&quot; f = (prm[2,nuc]+prm[3,nuc])/sum(prm[1:3,nuc])*100 paste(&quot;For&quot;,nuc,&quot;muons represent&quot;,round(f,1),&quot;% of SLHL surface production&quot;) ## [1] &quot;For Be10 muons represent 1.3 % of SLHL surface production&quot; Muons provide a a very small fraction of production at the surface for \\(^{10}\\)Be, but they penetrate much deeper than neutrons into rocks. For that reason they are going to account to a higher proportion of nuclides on eroding surfaces. We can separate the various sources of production (neutrons and muons) and see what is their respective contributions to the concentration observed at the surface. Code col_sp = &quot;deepskyblue&quot; col_sm = &quot;indianred1&quot; col_fm = &quot;darkseagreen1&quot; nuc = &quot;Be10&quot; data = data.frame(ero1=10^seq(log10(0.1),log10(2000),length=100)) # ero1 -&gt; denudation in m/Ma emin = min(data$ero1) emax = max(data$ero1) data$ero2 = data$ero1/1e6*100*rho # ero2 = denudation in g/cm2/a # steady state concentrations associated with individual production pathways data$Csp = prm[1,nuc]*S$Nneutrons/(prm[4,nuc]+(data$ero2/Lambda[1])) data$Csm = prm[2,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[2])) data$Cfm = prm[3,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[3])) data$C = data$Csp + data$Csm + data$Cfm # plot(NA,xlim=c(emin,emax),ylim=c(0,1),log=&quot;x&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Fraction&quot;,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) polygon(c(emin,emax,emax,emin),c(0,0,1,1),col=col_fm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev((data$Csp+data$Csm)/data$C)),col=col_sm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev(data$Csp/data$C)),col=col_sp) grid(col=&quot;black&quot;,equilogs = FALSE) # text(0.2,0.1,nuc,cex=2) legend(&quot;bottomright&quot;, c(&quot;Spallation&quot;,&quot;Stopping muons&quot;,&quot;Fast muons&quot;), pch=22,pt.bg=c(col_sp,col_sm,col_fm),pt.cex=1.5,cex=1,bg=&quot;white&quot;) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) col_sp = &quot;deepskyblue&quot; col_sm = &quot;indianred1&quot; col_fm = &quot;darkseagreen1&quot; nuc = &quot;Be10&quot; data = data.frame(ero1=10^seq(log10(0.1),log10(2000),length=100)) # ero1 -&gt; denudation in m/Ma emin = min(data$ero1) emax = max(data$ero1) data$ero2 = data$ero1/1e6*100*rho # ero2 = denudation in g/cm2/a # steady state concentrations associated with individual production pathways data$Csp = prm[1,nuc]*S$Nneutrons/(prm[4,nuc]+(data$ero2/Lambda[1])) data$Csm = prm[2,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[2])) data$Cfm = prm[3,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[3])) data$C = data$Csp + data$Csm + data$Cfm # plot(NA,xlim=c(emin,emax),ylim=c(0,1),log=&quot;x&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Fraction&quot;,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) polygon(c(emin,emax,emax,emin),c(0,0,1,1),col=col_fm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev((data$Csp+data$Csm)/data$C)),col=col_sm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev(data$Csp/data$C)),col=col_sp) grid(col=&quot;black&quot;,equilogs = FALSE) # text(0.2,0.1,nuc,cex=2) legend(&quot;bottomright&quot;, c(&quot;Spallation&quot;,&quot;Stopping muons&quot;,&quot;Fast muons&quot;), pch=22,pt.bg=c(col_sp,col_sm,col_fm),pt.cex=1.5,cex=1,bg=&quot;white&quot;) "],["two-nuclides-systems.html", "6 Two nuclides systems 6.1 Evolution of ratios 6.2 Steady-state denudation and constant exposure curves 6.3 Burial age and denudation rate curves 6.4 Exemple of Lombrive cave (Ariège) 6.5 Interactive shiny app", " 6 Two nuclides systems We first define the usual variables and parameters. Code data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.7 # we also define the density (g/cm3) altitude = 1000 # elevation in m latitude = 40 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Here first we consider that all samples are close and have similar positions and elevation, so similar scaling parameters. We will see later how to deal with situations where samples have different positions and hence different scaling parameters. In order to avoid messing up things later we define here once and for all which is nuclide 1 and which is nuclide 2, according to their respective \\(\\tau_{1/2}\\) : Code N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life 6.1 Evolution of ratios We first start by looking into the evolution of the isotopic ratio between the nuclides, once production has stopped, which is one of the key phenomenon used in burial dating applications. We will look at the evolution through time (typically over several Ma for the 26/10 system), starting from concentrations defined by a denudation steady-state. Code data = data.frame(t = seq(0,7e6,by=2000)) ero = 50 * rho * 100 / 1e6 # in g/cm2/a We compute the initial steady-state concentrations. Code C1_0 = solv_conc_eul(0,ero,Inf,0,prm[,N1],S,Lambda) C2_0 = solv_conc_eul(0,ero,Inf,0,prm[,N2],S,Lambda) Now we let them decay through time starting from the initial values, with no production (\\(z=+\\infty\\)). Code data$C1 = solv_conc_eul(Inf,0,data$t,C1_0,prm[,N1],S,Lambda) data$C2 = solv_conc_eul(Inf,0,data$t,C2_0,prm[,N2],S,Lambda) We plot the evolution of the ratio through time. Code plot(data$t,data$C2/data$C1,type=&quot;l&quot;,lwd=2,col=&quot;darkorange&quot;,xlab=&quot;Time (a)&quot;,ylab=paste(N2,&quot;/&quot;,N1)) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.7 # we also define the density (g/cm3) altitude = 1000 # elevation in m latitude = 40 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life data = data.frame(t = seq(0,7e6,by=2000)) ero = 50 * rho * 100 / 1e6 # in g/cm2/a C1_0 = solv_conc_eul(0,ero,Inf,0,prm[,N1],S,Lambda) C2_0 = solv_conc_eul(0,ero,Inf,0,prm[,N2],S,Lambda) data$C1 = solv_conc_eul(Inf,0,data$t,C1_0,prm[,N1],S,Lambda) data$C2 = solv_conc_eul(Inf,0,data$t,C2_0,prm[,N2],S,Lambda) plot(data$t,data$C2/data$C1,type=&quot;l&quot;,lwd=2,col=&quot;darkorange&quot;,xlab=&quot;Time (a)&quot;,ylab=paste(N2,&quot;/&quot;,N1)) Change the following parameters and observe the implication on the ratio and its evolution - location (altitude and latitude) - initial denudation rate (ero) 6.2 Steady-state denudation and constant exposure curves Two-nuclides plots are usually built around two curves representing the predictions, in terms of concentrations, for end-member simplified situations : steady-state denudation, where the concentrations of the nuclides of interest are computed for various values of surface denudation rates constant exposure, where the concentrations of the nuclides of interest are computed for various duration of surface exposition, with no denudation We will use the Eulerian point of view (function solv_conc_eul) to compute the concentrations, which is done by function tnp_curves. We compute the data for these two curves. Code tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho,dlim = c(0.01, 1000),alim = c(100, 1e+09)) ss_ero = tmp[[1]] cst_exp = tmp[[2]] The default parameters for the ranges of denudation rates and exposure ages can be adjusted if needed. Now that we can plot everything in a two-nuclides graph. While not mandatory, it is usually recommended to organize the plot this way : X-axis : longer half-life nuclide (our \\(N_1\\)) Y-axis : ratio of shorter to longer half-life nuclides (our \\(N_2/N_1\\)) Code plot(NA,xlim=range(cst_exp$C1,ss_ero$C1),ylim=range(1,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line lines(data$C1,data$C2/data$C1,lwd=2,col=&quot;darkorange&quot;) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.7 # we also define the density (g/cm3) altitude = 1000 # elevation in m latitude = 40 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life data = data.frame(t = seq(0,7e6,by=2000)) ero = 50 * rho * 100 / 1e6 # in g/cm2/a C1_0 = solv_conc_eul(0,ero,Inf,0,prm[,N1],S,Lambda) C2_0 = solv_conc_eul(0,ero,Inf,0,prm[,N2],S,Lambda) data$C1 = solv_conc_eul(Inf,0,data$t,C1_0,prm[,N1],S,Lambda) data$C2 = solv_conc_eul(Inf,0,data$t,C2_0,prm[,N2],S,Lambda) tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho,dlim = c(0.01, 1000),alim = c(100, 1e+09)) ss_ero = tmp[[1]] cst_exp = tmp[[2]] plot(NA,xlim=range(cst_exp$C1,ss_ero$C1),ylim=range(1,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line lines(data$C1,data$C2/data$C1,lwd=2,col=&quot;darkorange&quot;) We have also plotted the burial trajectory we generated above. Starting from a position on the steady-state denudation curve, we progressively lower the ratios. Note that you can adjust the extent of the curves by modifying the range in denudation rate (option dlim, default from 0.1 to 1000 m/Ma) in exposure duration (option alim, default from 100 a to 1 Ma). Again change the following parameters and observe the implications on plot - location (altitude and latitude) - initial denudation rate (ero) 6.3 Burial age and denudation rate curves It is also common practice in such plot to overlays network of curves, indicating for example burial age. The tnp_burial function allows to compute such curves. Code A = (1:10)*1e6 # increments in age (a) E = c(0.1,0.2,0.5,1,2,5,10,20,50,100,200,500,1000,2000) # increments in denudation rate (m/Ma) res = tnp_burial(A,E,prm[,N1],prm[,N2],Lambda,S,rho,n=100) # compute array We plot these lines on our two nuclides plot. Code plot(NA,xlim=range(cst_exp$C1,ss_ero$C1),ylim=range(0,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(res[[1]]$C1,res[[1]]$C2/res[[1]]$C1,col=&quot;black&quot;) # constant age lines(res[[2]]$C1,res[[2]]$C2/res[[2]]$C1,col=&quot;grey&quot;) # constant denudation lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line lines(data$C1,data$C2/data$C1,lwd=2,col=&quot;darkorange&quot;) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.7 # we also define the density (g/cm3) altitude = 1000 # elevation in m latitude = 40 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life data = data.frame(t = seq(0,7e6,by=2000)) ero = 50 * rho * 100 / 1e6 # in g/cm2/a C1_0 = solv_conc_eul(0,ero,Inf,0,prm[,N1],S,Lambda) C2_0 = solv_conc_eul(0,ero,Inf,0,prm[,N2],S,Lambda) data$C1 = solv_conc_eul(Inf,0,data$t,C1_0,prm[,N1],S,Lambda) data$C2 = solv_conc_eul(Inf,0,data$t,C2_0,prm[,N2],S,Lambda) tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho,dlim = c(0.01, 1000),alim = c(100, 1e+09)) ss_ero = tmp[[1]] cst_exp = tmp[[2]] A = (1:10)*1e6 # increments in age (a) E = c(0.1,0.2,0.5,1,2,5,10,20,50,100,200,500,1000,2000) # increments in denudation rate (m/Ma) res = tnp_burial(A,E,prm[,N1],prm[,N2],Lambda,S,rho,n=100) # compute array plot(NA,xlim=range(cst_exp$C1,ss_ero$C1),ylim=range(0,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(res[[1]]$C1,res[[1]]$C2/res[[1]]$C1,col=&quot;black&quot;) # constant age lines(res[[2]]$C1,res[[2]]$C2/res[[2]]$C1,col=&quot;grey&quot;) # constant denudation lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line lines(data$C1,data$C2/data$C1,lwd=2,col=&quot;darkorange&quot;) Comment on the general structure of the plot and our ability to resolve burial ages. 6.4 Exemple of Lombrive cave (Ariège) We are going to use data from Sartégou et al. (2020), obtained by measuring \\(^{10}Be\\) and \\(^{26}\\)Al concentrations in caves deposits from the Lombrives system, 114 m above the current level of the Ariège river. We start by loading the data (note that concentrations are in \\(\\times 10^3\\) at/g). Code data = read.table(&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/sartegou2020late.dat&quot;,header=T) data We also recompute the paramters according to location of Lombrive cave and the estimated average elevation of the paleo-catchment (see discussion in the article). Code data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.5 # we also define the density (g/cm3) altitude = 1990 # elevation in m latitude = 43 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) We also recompute the various reference curves of the plot. Code tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho,dlim = c(0.01, 1000),alim = c(1000, 1e+09)) ss_ero = tmp[[1]] cst_exp = tmp[[2]] # A = (1:10)*1e6 # increments in age (a) E = c(1,2,5,10,20,50,100,200,500,1000) # increments in denudation rate (m/Ma) res = tnp_burial(A,E,prm[,N1],prm[,N2],Lambda,S,rho,n=100) # compute array We now can plot the dataset. Code plot(NA,xlim=range(1e4,2e5),ylim=range(0.01,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(res[[1]]$C1,res[[1]]$C2/res[[1]]$C1,col=&quot;black&quot;) # constant age lines(res[[2]]$C1,res[[2]]$C2/res[[2]]$C1,col=&quot;grey&quot;) # constant denudation lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line # points(data$C10*1e3,data$C26/data$C10,pch=21,bg=&quot;pink&quot;,cex=2) text(data$C10*1e3,data$C26/data$C10,data$num,cex=0.5) Note that we have modified the X-axis extent with respect to previous plots. Analytical uncertainties are critical to interpret such plots, especially for long burial duration. We are going to plot them as ellispes, using the function tnp_ellipse. Code plot(NA,xlim=range(1e4,2e5),ylim=range(0.01,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(res[[1]]$C1,res[[1]]$C2/res[[1]]$C1,col=&quot;black&quot;) # constant age lines(res[[2]]$C1,res[[2]]$C2/res[[2]]$C1,col=&quot;grey&quot;) # constant denudation lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line # el = tnp_ellipse(data$C10*1e3, data$C10_e*1e3, data$C26*1e3, data$C26_e*1e3,confidence=0.68) for (i in 1:length(el)) {polygon(el[[i]],col=&quot;pink&quot;,border=&quot;grey&quot;)} text(data$C10*1e3,data$C26/data$C10,data$num,cex=0.5) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life data = read.table(&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/sartegou2020late.dat&quot;,header=T) data data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.5 # we also define the density (g/cm3) altitude = 1990 # elevation in m latitude = 43 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho,dlim = c(0.01, 1000),alim = c(1000, 1e+09)) ss_ero = tmp[[1]] cst_exp = tmp[[2]] # A = (1:10)*1e6 # increments in age (a) E = c(1,2,5,10,20,50,100,200,500,1000) # increments in denudation rate (m/Ma) res = tnp_burial(A,E,prm[,N1],prm[,N2],Lambda,S,rho,n=100) # compute array plot(NA,xlim=range(1e4,2e5),ylim=range(0.01,cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(res[[1]]$C1,res[[1]]$C2/res[[1]]$C1,col=&quot;black&quot;) # constant age lines(res[[2]]$C1,res[[2]]$C2/res[[2]]$C1,col=&quot;grey&quot;) # constant denudation lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line # el = tnp_ellipse(data$C10*1e3, data$C10_e*1e3, data$C26*1e3, data$C26_e*1e3,confidence=0.68) for (i in 1:length(el)) {polygon(el[[i]],col=&quot;pink&quot;,border=&quot;grey&quot;)} text(data$C10*1e3,data$C26/data$C10,data$num,cex=0.5) Comment on the implications for the burial ages and paleo-denudation rates. 6.5 Interactive shiny app You can also explore dynamically 2 nuclides plots using this embedded application, which use the same type of code. "],["depth-profiles.html", "7 Depth profiles 7.1 Setup and dataset 7.2 Profile modelling 7.3 Exploration of parameters space 7.4 Interactive shiny app", " 7 Depth profiles 7.1 Setup and dataset The package TCNtools includes a selection of depth profiles data from the literature. We will use the profile published in Laloy et al. (2017) from the Campines Plateau in Belgium. Code data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data We can then compute the scaling for this study site. Note that each site characteristics are only reported for the first sample (label=1). Code altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) We plot the depth profile Code plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce this figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Comment on the evolution of concentrations with depth. 7.2 Profile modelling We are going to use equation (3.1) to compute a theoretical concentration profile. We use initial guess of the value for the parameters Material density \\(\\rho\\) (rho) Surface denudation \\(\\varepsilon\\) (ero) Age of the deposit \\(t\\) (age) Inherited concentration at the time of deposition \\(C_0\\) (C0) Code rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ We now plot this modeled profile with the dataset. Code plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modelled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce this figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modelled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Try to get as close to the observations by modifying the various parameters. Comment on the influence of each parameter on the profile. 7.3 Exploration of parameters space We could change the parameters incrementally one by one to explore the effects on the modeled profile and attempt to converge toward a best-looking solution. It is obviously much more efficient and robust to do this automatically by going through a large number of parameters sets, compute the predicted concentrations (\\(C_{mod}\\)) and evaluate how well they match the measured concentrations (\\(C_{mes}\\) with uncertainty \\(\\sigma\\)). We can use the \\(\\chi^2\\) to quantify the difference between the model and observations. \\[\\begin{equation} \\chi^2 = \\sum_{i=1}^n \\left( \\frac{C_{mes,i} - C_{mod,i} }{\\sigma_i} \\right)^2 \\end{equation}\\] The function depth_profile_mc will allow us to generate a large number of models over the parameter space defined by : the age of the formation (age in a) the surface denudation rate (ero in m/Ma) the inherited concentration (inh in at/g) the density (rho in g/cm2) There two additional parameters n1 and n2, which control how many models we are going to sample from this parameters space n1 is the number of model which are randomly drawn (Monte Carlo) n2 is the number of regularly sampled points for each parameter (if all 4 parameters are varying the number of models is n2\\(^4\\)) We can choose to carry out an exploration of the parameters space which completely random (n2=0) or following a regular grid (n1=0), or a combination of both. This first attempt is a crude exploration of the parameter space. Code #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,100)*1000, ero=c(0,50), inh=c(0,1)*1e5, rho=c(1.5,2.5), n1=50e3) Note that in depth_profile_mc we could choose to fix a parameter by passing a single value as argument, instead of a two elements vector, which corresponds to the bounds of the explored interval. We can have a look at the result table. Code summary(res) ## age ero inh rho ## Min. : 2.54 Min. : 0.00231 Min. : 7.25 Min. :1.500 ## 1st Qu.: 25010.13 1st Qu.:12.55344 1st Qu.:25163.39 1st Qu.:1.746 ## Median : 49914.73 Median :25.08625 Median :49821.28 Median :1.997 ## Mean : 49944.71 Mean :25.02860 Mean :50014.50 Mean :1.998 ## 3rd Qu.: 75038.39 3rd Qu.:37.56844 3rd Qu.:75006.27 3rd Qu.:2.249 ## Max. : 99999.69 Max. :49.99916 Max. :99997.51 Max. :2.500 ## chi2 ## Min. : 13.09 ## 1st Qu.: 98.67 ## Median : 311.01 ## Mean : 459.33 ## 3rd Qu.: 712.99 ## Max. :2428.57 Based on the \\(\\chi2\\) value we can now try to have a look at a subset of our model, which correspond to the best fitting values for this metric. We can start to do that very coarsely by looking at a quantile, for example the best 1%. Code cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] pairs(best[,-5],pch=16,col=adjustcolor(&quot;blue&quot;,0.2)) # we remove the last column (5) which correspond to the chi2 We see numerous tradeoffs between parameters, notably between age and denudation rate. We can select and plot the best model Code imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe res[imin,] Code z = seq(0,max(data$depth),length.out = 40) # calculation depth (g/cm2) C = solv_conc_eul(z*res$rho[imin],res$ero[imin]*100/1e6*res$rho[imin],res$age[imin],res$inh[imin],prm[,&quot;Be10&quot;],S,Lambda) # compute concentration # plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modeled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce these figures Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,100)*1000, ero=c(0,50), inh=c(0,1)*1e5, rho=c(1.5,2.5), n1=50e3) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] pairs(best[,-5],pch=16,col=adjustcolor(&quot;blue&quot;,0.2)) # we remove the last column (5) which correspond to the chi2 imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe res[imin,] z = seq(0,max(data$depth),length.out = 40) # calculation depth (g/cm2) C = solv_conc_eul(z*res$rho[imin],res$ero[imin]*100/1e6*res$rho[imin],res$age[imin],res$inh[imin],prm[,&quot;Be10&quot;],S,Lambda) # compute concentration # plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modeled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Change the range of explored parameters Try to remove some samples, and observe the influence on the solution In many situations we have some knowledge about the values or plausible ranges of some parameters. For example the density can be measured at the sampling site, and inheritance can be estimated from the shape of the profile. In our case it seems reasonable to consider that it is close to 90\\(\\times10^3\\) at/g. We can restrict the range of variation or even fix these parameters, which is what we do next, by letting only the age and denudation rate as free parameters. Code #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,200)*1000, ero=c(0,50), inh=90*1e3, rho=2, n1=20000,n2=20) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] plot(best$ero,best$age/1000,pch=16,col=adjustcolor(&quot;blue&quot;,0.2), xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) In this case the results will be much easier to visualize through the construction of a \\(\\chi^2\\) surface. We use the library akima to grid the results (note that we use \\(\\log_{10}(\\chi^2)\\)). Code library(&quot;akima&quot;) spline&lt;-interp(res$ero/max(res$ero),res$age/max(res$age),log10(res$chi2),duplicate=&quot;mean&quot;,nx=100,ny=100) contour(spline$x*max(res$ero),spline$y*max(res$age)/1e3,spline$z,col=&quot;pink&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) # &quot;best&quot; model Below you can get all the necessary code to produce these figures Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,200)*1000, ero=c(0,50), inh=90*1e3, rho=2, n1=20000,n2=20) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] plot(best$ero,best$age/1000,pch=16,col=adjustcolor(&quot;blue&quot;,0.2), xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) library(&quot;akima&quot;) spline&lt;-interp(res$ero/max(res$ero),res$age/max(res$age),log10(res$chi2),duplicate=&quot;mean&quot;,nx=100,ny=100) contour(spline$x*max(res$ero),spline$y*max(res$age)/1e3,spline$z,col=&quot;pink&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) # &quot;best&quot; model Change the values for density and inheritance Repeat the calculations with the other profiles included in the dataset (data(\"tcn_depth_profiles\")) : Siame et al. (2004), Hein et al. (2009), Hidy et al. (2010) How well are the age and denudation rate defined for this profile? 7.4 Interactive shiny app You can also explore profile modelling dynamically using this embedded application, which use the same type of code. "],["references.html", "8 References", " 8 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "A Minimal Book Example 1 Prerequisites 1.1 Usage", " A Minimal Book Example Vincent Godard 2022-10-11 1 Prerequisites This html page is derived from an R Markdown Notebook. You can copy/paste the various lines of code into your own R script and run it in any R session. These activities do not require any specific prior knowledge of R programming language. The idea is for you to simply copy and paste the code in a script, run it, and change various parameters to observe and investigate the associated response. In addition to the scripting-oriented activities below you can also experiment visually with a few interactive Shiny apps http://shinyproxy.osupytheas.fr Both are built on the dedicated TCNtools package : https://vincentgodard.github.io/TCNtools This package contain various functions to assist the interpretation of TCN concentration at the Earth Surface. You can install the package directly on you system, using the instruction provided on the package webpage (requires devtools package to install from Github). Alternatively you can launch a binder session and get a RStudio session running into your browser with the TCNtools package installed, by clicking on the icon on the package website (right column). The first thing you will have to do, before calling any of the functions, is to load the TCNtools package. Code library(&quot;TCNtools&quot;) 1.1 Usage Various boxes will be used throughout the book. This a summary code box. In the following chapters the processes will explained step by step, with the various actions distributed between different successive code blocks (also named chunks) for better understanding. For convenience, and avoid you to have to copy and paste these various blocks one by one, sometimes everything will be combined in one block at the end of the section, which can unfold, copy and then paste into the R console or a script. Below you can get all the necessary code to produce a figure or result. Code a = 1 b = 3 c = (a+b)^2 The purpose of this book is to allow you to experiment and explore, not just copy and paste some code. This kind of box will suggest you some action on the code, typically changing the value of parameters to observe the evolution of the result. This box will be used to highlight or stress something important. This box will be used to formulate some questions for you to think about. "],["introduction.html", "2 Introduction 2.1 Objectives of the session", " 2 Introduction 2.1 Objectives of the session We are going to cover two things during this session look into the calculation of scaling factors used for the computation of local production rates, and how they vary in space and time make some simple calculations of TCN concentration build up at the Earth surface, as a response to exposure and denudation test "],["scaling-factors.html", "3 Scaling factors 3.1 Time-independent scaling 3.2 Time-dependent scalings 3.3 Interactive shiny app", " 3 Scaling factors First we are going to explore how the scaling factors are changing with elevation, latitude and time, and what is the impact on local production rates. 3.1 Time-independent scaling We are going to present the most widely used and simplest scaling scheme known as Lal-Stone and often abbreviated as st. The main equations are presented in the reference article by Stone (2000) . 3.1.1 Site characteristics We first need to define some parameters concerning the site of interest : latitude lat in degrees altitude z in meters (can be a vector or a scalar) longitude lon in degrees, this is not used for st scaling (Stone (2000)), just in case we want to compute atmospheric pressure according to ERA40 (Uppala et al. (2005)). Code lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments Now we can compute the atmospheric pressure, with the function atm_pressure according to the two models available, and then plot for comparison. Here z is a vector to see the variations over a range of elevations. To get information about the usage of the function used here (for example what are the different models) type ?atm_pressure in the R console. Code P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) Finally, we plot the results. Code plot(P1,z,type=&quot;l&quot;,xlab=&quot;Pressure (hPa)&quot;,ylab=&quot;Altitude (m)&quot;,col=&quot;darkorange3&quot;) lines(P2,z,lty=2,col=&quot;darkorange3&quot;) legend(&quot;topright&quot;,c(&quot;Stone 2000&quot;,&quot;ERA40&quot;),lty=c(1,2)) Modify lat and lon to see the effects on the pressure computed with the ERA40 model Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) plot(P1,z,type=&quot;l&quot;,xlab=&quot;Pressure (hPa)&quot;,ylab=&quot;Altitude (m)&quot;,col=&quot;darkorange3&quot;) lines(P2,z,lty=2,col=&quot;darkorange3&quot;) legend(&quot;topright&quot;,c(&quot;Stone 2000&quot;,&quot;ERA40&quot;),lty=c(1,2)) 3.1.2 Computation of scaling factors We can now compute the scaling factors according to Stone (2000). Same as above, to get some information about the function (parameters definition) type ?st_scaling in the R console. Code st = scaling_st(P1,lat) # here we use the pressure according to Stone 2000 model The result is stored in st as a dataframe with as many rows as there are elements in the input pressure vector (P1) and two columns named Nneutrons and Nmuons, for the spallogenic and muogenic contributions, respectively. Code st We can plot the evolution with elevation, which illustrates the major influence of altitude of the sampling site in controlling the local production rate. Code plot(st$Nneutrons,z,type=&quot;l&quot;, xlab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;,ylab=&quot;Altitude (m)&quot;, main=paste(&quot;Latitude &quot;,lat,&quot;°&quot;,sep=&quot;&quot;),col=&quot;darkorange3&quot;) Modify lat to see the effects on the scaling factor Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) lat = 30 # latitude lon = 30 # longitude z = seq(0,3000,by=100) # vector from 0 to 3000 m by 100 m increments P1 = atm_pressure(alt=z,model=&quot;stone2000&quot;) P2 = atm_pressure(alt=z,lat=lat,lon=lon,model=&quot;era40&quot;) st = scaling_st(P1,lat) # here we use the pressure according to Stone 2000 model plot(st$Nneutrons,z,type=&quot;l&quot;, xlab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;,ylab=&quot;Altitude (m)&quot;, main=paste(&quot;Latitude &quot;,lat,&quot;°&quot;,sep=&quot;&quot;),col=&quot;darkorange3&quot;) 3.1.3 Global variations In order to get a better idea of the variations with both latitude (from 0 to 90°) and elevation (from sea level to 3000 m) we can try the represent the evolution of the scaling factor for both paramters Code P = atm_pressure(alt=0,model=&quot;stone2000&quot;) # compute pressure lat = seq(0,90,by=1) # latitude vector n = length(lat) # size of vector # st = scaling_st(P,lat) # compute scaling at sea level plot(lat,st$Nneutrons,type=&quot;l&quot;,ylim=c(0.5,12),col=&quot;darkorange3&quot;, xlab=&quot;Latitude (°)&quot;,ylab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;) grid() text(lat[n],st$Nneutrons[n],&quot;0 km&quot;,cex=0.5,adj=0) # put label at the end of curve # for (z in seq(500,3000,by=500)){ # loop on elevations : same as above for a range of elevations P = atm_pressure(alt=z,model=&quot;stone2000&quot;) st = scaling_st(P,lat) lines(lat,st$Nneutrons,col=&quot;darkorange3&quot;) text(lat[n],st$Nneutrons[n],z/1000,cex=0.5,adj=0) } This dependence of the scaling factor on latitude is a direct consequence of the dipole structure of the Earth magnetic field, with a higher cosmic rays flux at high latitudes. Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) P = atm_pressure(alt=0,model=&quot;stone2000&quot;) # compute pressure lat = seq(0,90,by=1) # latitude vector n = length(lat) # size of vector # st = scaling_st(P,lat) # compute scaling at sea level plot(lat,st$Nneutrons,type=&quot;l&quot;,ylim=c(0.5,12),col=&quot;darkorange3&quot;, xlab=&quot;Latitude (°)&quot;,ylab=&quot;Spallogenic st scaling factor (Stone 2000)&quot;) grid() text(lat[n],st$Nneutrons[n],&quot;0 km&quot;,cex=0.5,adj=0) # put label at the end of curve # for (z in seq(500,3000,by=500)){ # loop on elevations : same as above for a range of elevations P = atm_pressure(alt=z,model=&quot;stone2000&quot;) st = scaling_st(P,lat) lines(lat,st$Nneutrons,col=&quot;darkorange3&quot;) text(lat[n],st$Nneutrons[n],z/1000,cex=0.5,adj=0) } 3.2 Time-dependent scalings 3.2.1 Definition of paleomagnetic variations Time-dependent scaling factors allow to take into account the variations through time of the Earth magnetic field, which modulates the incoming cosmic ray flux. This is particularly important in exposure dating applications. 3.2.1.1 Virtual Dipole Moment We need to first define a time series for the Virtual Dipole Moment (VDM) variation, using the get_vdm function. Several paleomagnetic database can be used. The three options correspond to databases defined in Crep. We first extract the values of the Virtual Dipole Moment at specified times (vector time), and then plot the result. Code time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) Try to modify the time vector and change the model option to lsd or musch 3.2.1.2 Cutoff Rigidity Now we need to convert that into cutoff rigidity using vdm2rc function. Such can be done using the following expression (Martin et al. (2017)): \\[R_c = 14.3 \\frac{M}{M_0}\\cos^4 \\lambda,\\] where \\(M\\) is the moment of the Earth dipole field, \\(M_0\\) the 2010 reference value for \\(M\\) and \\(\\lambda\\) the latitude. This corresponds to the default model=\"elsasser54\" in the vdm2rc function arguments. A more complex formula proposed by Lifton, Sato, and Dunai (2014) can be used with model=\"lifton14\". Code lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) Change the latitude lat and observe the influence on \\(R_c\\) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) 3.2.2 Lal/Stone modified scaling (lm) Once we have a \\(R_c\\) time series we can compute the lm scaling factors using the scaling_lm function. For that we will only use one elevation (z=0), so we recompute the atmospheric pressure. We plot the corresponding time series, as well as the value of st scaling factor for reference. Code P = atm_pressure(alt=0,model=&quot;stone2000&quot;) lm = scaling_lm(P,rc1) plot(time,lm,type=&quot;l&quot;,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Spallogenic lm scaling factor&quot;) abline(h=scaling_st(P,lat)$Nneutrons,lty=2) Explore the variations of the scaling factor by using various values for elevation alt, and different \\(R_c\\) time-series. Look for the differences with the time-independant st scaling Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) time = seq(0,50e3,length.out = 1000) # time vector from 0 to 50 ka BP, with 1000 regularly spaced elements vdm = get_vdm(time,model=&quot;glopis&quot;) plot(time,vdm,xlab=&quot;Time (a BP)&quot;,ylab=&quot;VDM (10^22 A.m^2)&quot;,type=&quot;l&quot;,col=&quot;coral&quot;) lat = 40 rc1 = vdm2rc(vdm,lat,model=&quot;elsasser54&quot;) rc2 = vdm2rc(vdm,lat,model=&quot;lifton14&quot;) # plot(time,rc1,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Rc (GV)&quot;,type=&quot;l&quot;,col=&quot;darkblue&quot;) lines(time,rc2,lty=2,col=&quot;darkblue&quot;) P = atm_pressure(alt=0,model=&quot;stone2000&quot;) lm = scaling_lm(P,rc1) plot(time,lm,type=&quot;l&quot;,xlab=&quot;Time (a BP)&quot;,ylab=&quot;Spallogenic lm scaling factor&quot;) abline(h=scaling_st(P,lat)$Nneutrons,lty=2) 3.3 Interactive shiny app You can also explore dynamically the behavior of scaling parameters using this embedded application, which use the same type of code. "],["exploring-tcn-build-up-at-the-surface.html", "4 Exploring TCN build up at the surface 4.1 Background 4.2 Set up of the calculations 4.3 Evolution of concentration with time 4.4 Two end-member situations 4.5 Interactive shiny app", " 4 Exploring TCN build up at the surface We are going to consider simple computations of concentration under various conditions in terms of erosion, depth or age. This will be done using an Eulerian point of view, which is the most straightforward and fastest way to perform such computation. In this case the quantity of interest (concentration) is computed at fixed depths below the surface, while the exhumed material is moving through this reference frame during its trajectory toward the surface. More details on the differences between Eulerian and Lagrangian approaches, and their applications to complex exposition/denudation histories, will be presented later. Note that interpreting measured concentrations in terms of end-member situations of pure exposure or steady-state denudation is often done with online calculators (Balco et al. (2008),Marrero et al. (2016),Martin et al. (2017)). Those calculators allow to perform very accurate computations of age or denudation rate, but one should always be careful about the underlying hypotheses (no erosion, steady state achieved, etc …) when interpreting measured concentrations and always think about how TCN are accumulating in the sampled material. The goal of this activity is to explore this behavior, using simple approaches. 4.1 Background The relevant general equation is the following, \\[\\begin{equation} C=C_0e^{-\\lambda t} + \\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda}e^{\\frac{-\\rho z}{\\Lambda_i}}(1-e^{-(\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda)t}) \\tag{4.1} \\end{equation}\\] with the following variables and parameters, \\(C\\) the concentration (as a function of time \\(t\\) and depth \\(z\\)) \\(C_0\\) the inherited concentration \\(\\lambda\\) the decay constant for the considered nuclide \\(P_i\\) the scaled surface production rate for the nuclide of interest and the \\(i\\)-th production pathway (spallation, stopped muons, fast muons) \\(\\rho\\) the density of the medium \\(\\Lambda_i\\) the attenuation length for the particules of the \\(i\\)-th production pathway \\(\\varepsilon\\) surface denudation In order to stick with usual conventions in the following time \\(t\\) will be measured in years (a), the unit of length will be cm and the depths (\\(z\\)) will be expressed in g/cm\\(^2\\) (i.e. actual depth \\(\\times \\rho\\)). Note two keys limitations of this representation : it does not allow to account for time variations of production rates (at least in its most straightforward implementation), so we will mostly using the st scaling it assumes exponential evolution of production with depth, which is clearly not the case for low energy neutrons (figure 2b from Gosse and Phillips (2001)) and is questionable in some situations for muons (Balco (2017)) 4.2 Set up of the calculations We should introduce some of the basic parameters we are going to use for the computation. For easy reference a set of data for parameters of interest is included in the TCNtools package. a vector (Lambda) with the attenuation lengths for different particles (in g/cm\\(^2\\)) neutrons for spallation reactions \\(\\Lambda_{spal}\\) stopping muons \\(\\Lambda_{stop}\\) fast muons \\(\\Lambda_{fast}\\) a vector (prm) with the SLHL production rates (in at/g/a), in this case for the st scaling scheme (Stone (2000)), and decay constant \\(\\lambda\\) (in 1/a) for the nuclides of interest. Note that this will need to be modified when using other scaling schemes. Note that these often used SLHL values are defined for convenience, most calculators for exposure age work directly with the production rate value at calibration sites, and that they are always relative to the scaling scheme used (Borchers et al. (2016)). We can first load the attenuation length data (g/cm\\(^2\\)). Documentation of this dataset is accessible by typing ?Lambda in the R console. Code data(Lambda) # we load a vector containing the attenuation length into the environment print(Lambda) ## Lspal Lstop Lfast ## 160 1500 4320 Code rho = 2.7 # we also define the density (g/cm3) Some production and decay parameters can also be loaded. Documentation of this dataset is accessible with ?prm. Code data(prm) # we load a matrix containing the production/decay parameters into the environment print(prm) ## Be10 Al26 C14 ## Pspal 4.01000e+00 2.793000e+01 1.224000e+01 ## Pstop 1.20000e-02 8.400000e-01 3.310000e+00 ## Pfast 3.90000e-02 8.100000e-02 0.000000e+00 ## lambda 5.09667e-07 9.667325e-07 1.209681e-04 We also need to define the properties of our site of interest and compute the relevant scaling parameters. As we already saw previously, this can easily be done with, Code altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) 4.3 Evolution of concentration with time To get a general overview of the behavior we are going to use directly the solv_conc_eul function, which allows to easily deal with various scenarios and configurations, using equation (4.1). In the following chapters we will go back to the key equations to get a better sense of the importance of various parameters. As always the documentation of the function, including its various arguments, can be obtained by typing ?solv_conc_eul in the R console. We start by defining the various Code nuc = &quot;Be10&quot; # &quot;Al26&quot;, &quot;C14&quot; t = seq(0,200e3,length.out=1000) # a vector containing time from 0 to 100 ka by 100 a steps z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration (at/g) ero = 0 * (100/1e6*rho) # denudation rate expressed in m/Ma and converted in g/cm2/a Now we can compute the concentration (at various times t), according to equation (4.1). Code C = solv_conc_eul(z,ero,t,C0,prm[,nuc],S,Lambda) # compute concentration Then we can plot the evolution of concentration with time. Code plot(t/1000,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # see explanation for these lines below Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions below) lambda = prm[4,nuc] # radiactive decay abline(0,sum(Prod)*1000,lty=2) # note that time is in ka on the plot abline(h=sum(Prod/((ero/Lambda)+lambda)),lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; # &quot;Al26&quot;, &quot;C14&quot; t = seq(0,200e3,length.out=1000) # a vector containing time from 0 to 100 ka by 100 a steps z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration (at/g) ero = 0 * (100/1e6*rho) # denudation rate expressed in m/Ma and converted in g/cm2/a C = solv_conc_eul(z,ero,t,C0,prm[,nuc],S,Lambda) # compute concentration plot(t/1000,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # see explanation for these lines below Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions below) lambda = prm[4,nuc] # radiactive decay abline(0,sum(Prod)*1000,lty=2) # note that time is in ka on the plot abline(h=sum(Prod/((ero/Lambda)+lambda)),lty=2) Starting with zero erosion (\\(\\varepsilon=0\\)), corresponding to the pure exposition of a surface, we see here the progressive build-up of concentration though time and the establishment of balance between gains (production) and losses (denudation and decay) leading to the concentration plateau at steady state. Note that two dashed lines are added to the graph, The first one corresponds to the production slope \\(\\sum_i P_i\\), how much nuclide you produce and how it would accumulate if you had no radioactive decay and no denudation. The second one (horizontal, not visible with the initial parameters) is the maximum value of concentration when steady state is achieved : \\[ C_{max}=\\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda} \\] Change the maximum of the t vector until you see the influence of radioactive decay and the plateau. Add some inheritance (\\(C_0\\)) Test the evolution with other nuclides such as \\(^{26}\\)Al and \\(^{14}\\)C (set nuc to Al26 or C14) Modify the ero parameter above (always keeping it is in g/cm\\(^2\\)/a), to see its influence on time needed to reach steady state and the final concentration 4.4 Two end-member situations Now we are going to build a summary plot showing the influence of both exposure and denudation. Code nuc = &quot;Be10&quot; # choice of nuclide t = 10^seq(log10(1),log10(10e6),length.out=1000) # time vector, log-spaced! # calculation of the evolution of concentration for denudation = 0 C = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure plot(t,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (a)&quot;,log=&quot;xy&quot;) grid() text(max(t),max(C),0,cex=0.5,adj=0) # label the curve # now we make the same computation for other denudation rates (using a loop) ero = c(1,10,100,1000) # erosion vector in m/Ma for (i in 1:length(ero)){ e = ero[i] * (100/1e6*rho) # convert denudation in g/cm2/a C = solv_conc_eul(0,e,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t,C,col=&quot;cornflowerblue&quot;,lwd=3) text(max(t),max(C),ero[i],cex=0.5,adj=0) # label the curve } Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; # choice of nuclide t = 10^seq(log10(1),log10(10e6),length.out=1000) # time vector, log-spaced! # calculation of the evolution of concentration for denudation = 0 C = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure plot(t,C,type=&quot;l&quot;,col=&quot;cornflowerblue&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (a)&quot;,log=&quot;xy&quot;) grid() text(max(t),max(C),0,cex=0.5,adj=0) # label the curve # now we make the same computation for other denudation rates (using a loop) ero = c(1,10,100,1000) # erosion vector in m/Ma for (i in 1:length(ero)){ e = ero[i] * (100/1e6*rho) # convert denudation in g/cm2/a C = solv_conc_eul(0,e,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t,C,col=&quot;cornflowerblue&quot;,lwd=3) text(max(t),max(C),ero[i],cex=0.5,adj=0) # label the curve } Note that this a log-log plot. It is probably one of the most important figure to keep in mind when analyzing TCN concentrations. It clearly shows the existence of two end-member situations when interpreting these concentrations, in terms of exposure age or denudation rates, and the transition between the two. Think about a bit about the following points What are the key hypothesis made when interpreting a TCN concentration in terms of exposure age surface denudation Can you think of geological/geomorphological situations where these hypotheses are violated? Why are the plateau concentrations so different? How is the time to reach the plateau changing and why? 4.5 Interactive shiny app You can also explore dynamically the TCN build up using this embedded application, which use the same type of code. "],["application-to-exposure-dating.html", "5 Application to exposure dating 5.1 Back to the evolution of concentration 5.2 Interpretation in terms of exposure ages 5.3 Time varying production rates 5.4 Dealing with uncertainties", " 5 Application to exposure dating 5.1 Back to the evolution of concentration We first consider the evolution of concentration with time \\(t\\). The computation will be carried out at the surface (\\(z=0\\)), but this could be done at any arbitrary depth. We also consider that \\(\\varepsilon = 0\\) and that there is no inheritance. The starting equation above becomes, \\[\\begin{equation} C(t)=\\sum_i \\frac{P_i}{\\lambda}(1-e^{-\\lambda t}) \\tag{5.1} \\end{equation}\\] For the sake of the example we are going to compute that by hand, and compare with the results obtained with the solv_conc_eul function, Again we define the usual parameters Code # set up altitude = 1000 # elevation in m latitude = 45 # latitude in degrees nuc = &quot;Be10&quot; P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) To improve readability we compute scaled production rates and give them explicit names. Code Pspal = prm[1,nuc]*S$Nneutrons # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,nuc]*S$Nmuons # scaled stopped muons production rate in at/g/y Pfast = prm[3,nuc]*S$Nmuons # scaled fast muons production rate in at/g/y lambda = prm[4,nuc] # radioactive decay (1/y) And then apply equation (5.1) and compare the results with the output of function solv_conc_eul. Code t = seq(0,200e3,by=100) # a vector containing time from 0 to 100 ka by 100 a steps C = (Pspal + Pstop + Pfast) / lambda * (1-exp(-lambda*t)) plot(t/1000,C,type=&quot;l&quot;,col=&quot;coral&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # check if this is ok C2 = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t/1000,C2,lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # # set up altitude = 1000 # elevation in m latitude = 45 # latitude in degrees nuc = &quot;Be10&quot; P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Pspal = prm[1,nuc]*S$Nneutrons # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,nuc]*S$Nmuons # scaled stopped muons production rate in at/g/y Pfast = prm[3,nuc]*S$Nmuons # scaled fast muons production rate in at/g/y lambda = prm[4,nuc] # radioactive decay (1/y) t = seq(0,200e3,by=100) # a vector containing time from 0 to 100 ka by 100 a steps C = (Pspal + Pstop + Pfast) / lambda * (1-exp(-lambda*t)) plot(t/1000,C,type=&quot;l&quot;,col=&quot;coral&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Time (ka)&quot;) grid() # check if this is ok C2 = solv_conc_eul(0,0,t,0,prm[,nuc],S,Lambda) # compute concentration for pure exposure lines(t/1000,C2,lty=2) One thing to note in this graph is that time \\(t\\) is expressed as time elapsed since start of exposure, which we can not directly translate into the age BP, when dealing with radioactive decay and time variations in production rate. Change the maximum time t to check that everything is correct. 5.2 Interpretation in terms of exposure ages We are now going to make some simple calculations to interpret concentrations in terms of exposure age. The objective is solving for \\(t\\) the equation \\(C_{mod}(t) = C_{mes}\\). The left hand side is our modelled concentration, obtained for example with equation (4.1) (solv_conc_eul), where we need to keep in mind all the hypotheses we make (\\(z=0\\), \\(\\varepsilon=0\\), \\(C_0=0\\)). We are going to use an example from Protin et al. (2019), using sample ARG-16-9, which was collected from a lateral moraine of the Argentière glacier (Northern French Alps). Here are the characteristics of this sample (see table 1 from this paper). Code rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Now, for example we can make some guess about the exposure age and see what is the difference with the measured concentration. Code age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration Below you can get all the necessary code to compute Cmod Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration Change the value of age and try to get close to measured \\(^{10}\\)Be concentration. We could try make Cmod equal to Cmes by trial and error, but we can be more systematic. We go through automatically over a whole range of age values, and compute the corresponding Cmod values. Code age = seq(0,20e3,by=10) # we define a vector for age, which set the different explored values Cmod = rep(NA,length(age)) # The vector of same length to store the results for (i in 1:length(age)){ Cmod[i] = solv_conc_eul(0,0,age[i],0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration } We then plot the results to identify the minimum value of \\(|C_{mod} - C_{mes}|\\) Code imin = which.min(abs(Cmod-Cmes)) # what is the index of the minimum difference res = age[imin] # the corresponding age plot(age,Cmod-Cmes,type=&quot;l&quot;,main=paste(res,&quot;a BP&quot;),col=&quot;coral&quot;,lwd=2) abline(h=0) abline(v=res) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) age = 10e3 # a Cmod = solv_conc_eul(0,0,age,0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration age = seq(0,20e3,by=10) # we define a vector for age, which set the different explored values Cmod = rep(NA,length(age)) # The vector of same length to store the results for (i in 1:length(age)){ Cmod[i] = solv_conc_eul(0,0,age[i],0,prm[,&quot;Be10&quot;],S*Ss,Lambda) # compute concentration } imin = which.min(abs(Cmod-Cmes)) # what is the index of the minimum difference res = age[imin] # the corresponding age plot(age,Cmod-Cmes,type=&quot;l&quot;,main=paste(res,&quot;a BP&quot;),col=&quot;coral&quot;,lwd=2) abline(h=0) abline(v=res) Compare with the age reported in table 1 from Protin et al. (2019). Comment about the possible causes for the differences? We can be even more efficient by using an simple optimization approach to solve \\(C_{mod}(t) = C_{mes}\\), using the built in optimize function. We just define a function to be optimized (search of minimum) and launch a search over an age range. We are looking for the minimum value of \\(|C_{mod}(t)-C_{mes}|\\). Here the function we are trying to optimize. Code fun_opt &lt;-function(t,Cmes,prm,S,Lambda){ Cmod = solv_conc_eul(0,0,t,0,prm[,&quot;Be10&quot;],S,Lambda) return(abs(Cmod-Cmes)) } We then launch the search of the 0-50 ka interval. Code res = optimize(fun_opt,c(0,50e3),Cmes,prm,S*Ss,Lambda) print(res) ## $minimum ## [1] 12478.57 ## ## $objective ## [1] 0.0008513347 Below you can get all the necessary code to obtain this result Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) fun_opt &lt;-function(t,Cmes,prm,S,Lambda){ Cmod = solv_conc_eul(0,0,t,0,prm[,&quot;Be10&quot;],S,Lambda) return(abs(Cmod-Cmes)) } res = optimize(fun_opt,c(0,50e3),Cmes,prm,S*Ss,Lambda) print(res) This is a very coarse calculation, but searching the solution for \\(|C_{mod}(t)-C_{mes}|\\) is the essence of what online calculator do. What kind of improvement could you think of concerning this calculation? 5.3 Time varying production rates We are now going back to the time varying-scaling schemes, such as the one we explored in the previous chapter. Code data = data.frame(t1=seq(0,20e3,length.out=2000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,latitude,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc) plot(data$t2,data$lm,type=&quot;l&quot;,xlab=&quot;Age BP (a)&quot;,ylab=&quot;Scaling factor&quot;) abline(h=S$Nneutrons,col=&quot;red&quot;) abline(h=mean(data$lm),lty=2) In this case we can not use the same equations for the evolution of TCN concentration as above, because we need to account for \\(P(t)\\). We are going to use, \\[C_{mod}(t) = e^{-\\lambda t} \\int_0^t P(t&#39;)e^{\\lambda t&#39;}dt&#39;\\] Note : it is usually considered that production by muons is much less affected than that by neutrons by the magnetic field variations. Below we first compare how the concentration evolves through time with the two approaches. Code library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # Pspal = prm[1,&quot;Be10&quot;]*S$Nneutrons*Ss # scaled spallation production rate in at/g/y (st scaling) Pstop = prm[2,&quot;Be10&quot;]*S$Nmuons*Ss # scaled stopped muons production rate in at/g/y Pfast = prm[3,&quot;Be10&quot;]*S$Nmuons*Ss # scaled fast muons production rate in at/g/y lambda = prm[4,&quot;Be10&quot;] # radioactive decay (1/y) data$Prod_st = Pspal + Pstop + Pfast data$C1 = solv_conc_eul(0,0,data$t1,0,prm,S*Ss,Lambda) # data$Prod_lm = prm[1,&quot;Be10&quot;]*data$lm*Ss+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*Ss*S$Nmuons data$C2 = exp(-lambda*data$t1)*cumtrapz(data$t1,data$Prod_lm*exp(lambda*data$t1)) # plot(data$t1,data$C1,type=&quot;l&quot;,xlab=&quot;Time since start of exposure (a)&quot;,ylab=&quot;Concentration (at/g)&quot;,col=&quot;cyan4&quot;) lines(data$t1,data$C2,col=&quot;darkgoldenrod2&quot;) legend(&quot;topleft&quot;,c(&quot;st&quot;,&quot;lm&quot;),lty=1,col=c(&quot;cyan4&quot;, &quot;darkgoldenrod2&quot;)) Comment on the differences. We can solve solve \\(C_{mod}(t)=C_{mes}\\) for \\(t\\) with the same approach as the st case, Code library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # compute_C&lt;-function(t1,prm,P,lat,Lambda,S,Ss){ data = data.frame(t1=seq(0,t1,length.out=1000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,lat,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc)*Ss Cmod = exp(-prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1[nrow(data)])*trapz(data$t1,(prm[1,&quot;Be10&quot;]*data$lm+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*S$Nmuons)*(exp(prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1))) return(Cmod) } # fun_opt2 &lt;-function(t1,Cmes,prm,P,lat,Lambda,S,Ss){ Cmod = compute_C(t1,prm,P,lat,Lambda,S,Ss) return(abs(Cmod-Cmes)) } # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) res = optimize(fun_opt2,c(0,50e3),Cmes,prm,P,latitude,Lambda,S,Ss) print(res) ## $minimum ## [1] 12562.66 ## ## $objective ## [1] 0.001908477 Below you can get all the necessary code to obtain this result Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) rho = 2.7 # rho = 2.7 altitude = 2252 # elevation in m latitude = 45.97 # latitude in degrees longitude = 6.96 # longitude in degrees Ss = 0.939*exp(-2.6*rho/Lambda[1]) # factor accounting for topographic shielding and sample thickness Cmes = 26.8e4 # measured concentration (at/g) Cmes_e = 1.3e4 # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) library(pracma) # useful library containing the cumtrapz function for trapezoidal integration # compute_C&lt;-function(t1,prm,P,lat,Lambda,S,Ss){ data = data.frame(t1=seq(0,t1,length.out=1000)) # we build a dataframe to store results data$t2 = max(data$t1) - data$t1 # time BP data$vdm = get_vdm(data$t2,model=&quot;musch&quot;) data$rc = vdm2rc(data$vdm,lat,model=&quot;elsasser54&quot;) data$lm = scaling_lm(P,data$rc)*Ss Cmod = exp(-prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1[nrow(data)])*trapz(data$t1,(prm[1,&quot;Be10&quot;]*data$lm+(prm[2,&quot;Be10&quot;] + prm[3,&quot;Be10&quot;])*S$Nmuons)*(exp(prm[&quot;lambda&quot;,&#39;Be10&#39;]*data$t1))) return(Cmod) } # fun_opt2 &lt;-function(t1,Cmes,prm,P,lat,Lambda,S,Ss){ Cmod = compute_C(t1,prm,P,lat,Lambda,S,Ss) return(abs(Cmod-Cmes)) } # P = atm_pressure(alt=altitude,lat=latitude,lon=longitude,model=&quot;era40&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) res = optimize(fun_opt2,c(0,50e3),Cmes,prm,P,latitude,Lambda,S,Ss) print(res) Caveat we did those calculation using a number of simplification and numerical shortcuts …. 5.4 Dealing with uncertainties "],["application-to-denudation-rate-measurements.html", "6 Application to denudation rate measurements 6.1 Concentration - denudation rate relationship 6.2 Integration time scale 6.3 Catchment wide denudation rates 6.4 Change in denudation through time 6.5 On the importance of muons", " 6 Application to denudation rate measurements 6.1 Concentration - denudation rate relationship Now we are going to consider the evolution of concentration with denudation rate \\(\\varepsilon\\). The computation will be carried out at the surface (\\(z=0\\)), but this could be done at any arbitrary depth. We will consider that \\(t=+\\infty\\) and that we have reached the plateau concentration. Equation (4.1) becomes, \\[\\begin{equation} C=\\sum_i \\frac{P_i}{\\frac{\\rho \\varepsilon}{\\Lambda_i}+\\lambda} \\tag{6.1} \\end{equation}\\] Which simplifies to \\(C\\approx\\frac{P_i \\Lambda_i}{\\rho \\varepsilon}\\) if we neglect radioactive decay. We define the usual parameters. Code altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Now compute the steady-state concentration for a range of denudation rates. Code nuc = &quot;Be10&quot; ero = 10^seq(log10(0.1),log10(1000),length.out = 100) * 100/1e6*rho # a log-spaced vector for denudation rate expressed in m/Ma and converted in g/cm2/a age = Inf # infinite age z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration C = solv_conc_eul(z,ero,age,C0,prm[,nuc],S,Lambda) # compute concentration We plot the result, and also consider consider the implications of neglecting the radioactive decay. Code plot(ero/100*1e6/rho,C,col=&quot;lawngreen&quot;,log=&quot;xy&quot;,type=&quot;l&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) # what happens if we neglect radioactive decay Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions) lambda = prm[4,nuc] # radioactive decay C2 = sum(Prod*Lambda)/ero lines(ero/100*1e6/rho,C2,lty=2) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) nuc = &quot;Be10&quot; ero = 10^seq(log10(0.1),log10(1000),length.out = 100) * 100/1e6*rho # a log-spaced vector for denudation rate expressed in m/Ma and converted in g/cm2/a age = Inf # infinite age z = 0 * rho # depth at which we are going to perform the calculation (cm converted to g/cm2) C0 = 0 # inherited concentration C = solv_conc_eul(z,ero,age,C0,prm[,nuc],S,Lambda) # compute concentration plot(ero/100*1e6/rho,C,col=&quot;lawngreen&quot;,log=&quot;xy&quot;,type=&quot;l&quot;,lwd=3,ylab=&quot;Concentration (at/g)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) # what happens if we neglect radioactive decay Prod = c(prm[1,nuc]*S$Nneutrons,prm[2,nuc]*S$Nmuons,prm[3,nuc]*S$Nmuons) # scaled production vector (defined for the sake of clarity of the expressions) lambda = prm[4,nuc] # radioactive decay C2 = sum(Prod*Lambda)/ero lines(ero/100*1e6/rho,C2,lty=2) This figure (log-scales on both axes) highlights the strong inverse relationship, at steady-state, between denudation rate (\\(\\varepsilon\\)) and concentration (\\(C\\)), which is the foundation of many geomorphological studies trying to establish landscape evolution rates. Note the change in the relationship at very low denudation rates, which corresponds to the situation where the effects of radioactive decay become predominant. Over what range of denudation rates it is reasonable to neglect radioactive decay? What kind of geological context could it correspond to? A simple way to answer this question would be to compute the relative difference between the computed concentrations Code error = abs(C-C2)/C*100 plot(ero/100*1e6/rho,error,log=&quot;xy&quot;,type=&quot;l&quot;,ylab=&quot;Relative error (%)&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;) 6.2 Integration time scale Like all techniques used to measure erosion or denudation of the Earth surface, rates measured from cosmogenic nuclides are averaged over a specific period of time. This integration time-scale is often considered, at first order, to be the time required to erode the thickness of the zone where most of nuclide production occurs (von Blanckenburg (2005)), which can be considered as the penetration length for neutrons \\(\\Lambda/\\rho\\). This average is one of the advantages of the method, allowing to filter the high frequency variability, in particular of anthropic origin (Reusser, Bierman, and Rood (2015)). In its simplest form the integration time scale \\(\\tau\\) for a denudation rate measurement can be calculated as, \\[\\begin{equation} \\tau = \\frac{\\Lambda}{\\rho \\varepsilon} \\end{equation}\\] Code rho = 2.7 data(Lambda) ero = 10^seq(log10(0.5),log10(2000),length.out = 100) # log spaced vector for denudation rate mm/ka tau = (Lambda[1]/rho*10)/ero plot(ero,tau,xlab=&quot;Denudation rate (mm/ka)&quot;,ylab=&quot;Integration time scale (ka)&quot;,type=&quot;l&quot;,log=&quot;xy&quot;,lwd=3,col=&quot;gold&quot;) This simple plot allows to observe the very strong influence of denudation rate \\(\\varepsilon\\) on the integration time scale. For a denudation rate of 10 mm/ka, typical of slow erosion cratonic domains, the integration time scale is ~60 ka For a denudation rate of 500 mm/ka, which can be encountered in high relief mountain ranges, the integration time scale is ~1 ka What are the implications of this relationship for the geomorphological interpretation of the denudation rates? 6.3 Catchment wide denudation rates It can be noted that when dealing with eroding bedrock surfaces the calculated denudation rate will be a very local estimate and that its extrapolation to longer wavelengths is only possible in the rather limited cases where the morphological properties of the relief are uniform over large distances (plateaus, morphological surfaces, etc …). In reality, in complex environments, such as mountain ranges, where several processes contribute to denudation, a simple local estimate does not provide really useful information on the overall dynamics of the topography. An alternative approach is to use river sediments, which are a mixture of contributions from different parts of a watershed. The measurement of a cosmogenic nuclide concentration in this type of sample provides an estimate of the average denudation over the corresponding basin (Brown et al. (1995),Granger, Kirchner, and Finkel (1996),von Blanckenburg (2005)). Calculating such average denudation rate requires to obtain an average production rate for the catchment. For low relief catchments the scaling parameters can be calculated using the average elevation of the catchment. But for catchments with several 1000s m in relief, the non linear relationship between the scaling parameters and elevation will make this approximation invalid. We are going to explore these ideas using 3 basins from the study of Godard et al. (2012) in the Marsyandi river basin (central Nepal) Here are the maps for these 3 basins. We are going to load the corresponding data as dataframes (x, y, z), either from the disk or directly from the github repository (path_to_data indicates where to look for the data). Code #path_to_data = &quot;data/gis/&quot; path_to_data =&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/&quot; # we download directly from the github repository names_basins = c(&quot;Marsyandi&quot;,&quot;Chudi&quot;,&quot;Khudi&quot;) cols = c(&quot;deepskyblue&quot;, &quot;khaki3&quot;, &quot;darkolivegreen3&quot;) basins = list() # we store the dataframe into a list object for (i in 1:length(names_basins)){ basins[[i]] = read.table(paste0(path_to_data,names_basins[i],&quot;.dat&quot;),header=T) } We now plot the distribution of elevation for the 3 basins. Code # for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$z),ylim=c(0,0.0022),xlab=&quot;Elevation (m)&quot;,col=cols[i],main=&quot;&quot;) }else{ # then add the following basins as lines on the initial plot lines(density(basins[[i]]$z),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) Compare the distributions of elevation, and their position across the Himalayan range. How is this going to impact the determination of an average production rate for the basin? We then compute the st scaling parameters for each pixel (lines in the dataframes). Code for (i in 1:length(names_basins)){ basins[[i]]$P = atm_pressure(alt=basins[[i]]$z,model=&quot;stone2000&quot;) st = scaling_st(basins[[i]]$P,basins[[i]]$y) basins[[i]] = cbind(basins[[i]],st) } We plot the distribution of the spallation scaling factor for each basin. Code # plotting result for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$Nneutrons),ylim=c(1e-4,2.5),xlab=&quot;Spallation scaling factor&quot;,col=cols[i],main=&quot;&quot;,log=&quot;y&quot;) }else{ lines(density(basins[[i]]$Nneutrons),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) We now compute average values for the scaling paramters, in 2 different ways, 1 : We compute the scaling at each pixel according to its latitude and altitude (as we did above), and then take the average over the basin 2 : we compute the average latitude and altitude of the basin, and then use these average values to compute the scaling factors We create a dataframe res to store the results. Code res = data.frame(names=names_basins) # dataframe to store the results res$S1 = NA res$S2 = NA res$relief = NA for (i in 1:nrow(res)){ res$relief[i] = max(basins[[i]]$z) - min(basins[[i]]$z) # res$S1[i] = mean(basins[[i]]$Nneutrons) # P2 = atm_pressure(alt=mean(basins[[i]]$z),model=&quot;stone2000&quot;) st = scaling_st(P2,mean(basins[[i]]$y)) res$S2[i] = st$Nneutrons } print(res) ## names S1 S2 relief ## 1 Marsyandi 12.663441 9.444905 7676.423 ## 2 Chudi 1.420476 1.403903 1220.326 ## 3 Khudi 5.841666 5.168450 4065.374 We can visualize the difference between the two approaches. Code plot(res$S2,res$S1,pch=21,bg=cols, xlab=&quot;S2 : Scaling from average elevation&quot;,ylab=&quot;S1 : Scaling pixel by pixel&quot;, xlim=range(0,res$S2,res$S1),ylim=range(0,res$S2,res$S1)) grid() text(res$S2,res$S1,res$names,cex=0.5,pos=1) abline(0,1,lty=2) Below you can get all the necessary code to produce the figures presented in this section Code library(&quot;TCNtools&quot;) #path_to_data = &quot;data/gis/&quot; path_to_data =&quot;https://raw.githubusercontent.com/VincentGodard/TCNbook/main/data/gis/&quot; # we download directly from the github repository names_basins = c(&quot;Marsyandi&quot;,&quot;Chudi&quot;,&quot;Khudi&quot;) cols = c(&quot;deepskyblue&quot;, &quot;khaki3&quot;, &quot;darkolivegreen3&quot;) basins = list() # we store the dataframe into a list object for (i in 1:length(names_basins)){ basins[[i]] = read.table(paste0(path_to_data,names_basins[i],&quot;.dat&quot;),header=T) } # for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$z),ylim=c(0,0.0022),xlab=&quot;Elevation (m)&quot;,col=cols[i],main=&quot;&quot;) }else{ # then add the following basins as lines on the initial plot lines(density(basins[[i]]$z),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) for (i in 1:length(names_basins)){ basins[[i]]$P = atm_pressure(alt=basins[[i]]$z,model=&quot;stone2000&quot;) st = scaling_st(basins[[i]]$P,basins[[i]]$y) basins[[i]] = cbind(basins[[i]],st) } # plotting result for (i in 1:length(names_basins)){ if (i == 1){ # we initiate a plot at the first iteration plot(density(basins[[i]]$Nneutrons),ylim=c(1e-4,2.5),xlab=&quot;Spallation scaling factor&quot;,col=cols[i],main=&quot;&quot;,log=&quot;y&quot;) }else{ lines(density(basins[[i]]$Nneutrons),col=cols[i]) } } legend(&quot;topright&quot;,names_basins,cex=0.5,lty=1,col=cols) res = data.frame(names=names_basins) # dataframe to store the results res$S1 = NA res$S2 = NA res$relief = NA for (i in 1:nrow(res)){ res$relief[i] = max(basins[[i]]$z) - min(basins[[i]]$z) # res$S1[i] = mean(basins[[i]]$Nneutrons) # P2 = atm_pressure(alt=mean(basins[[i]]$z),model=&quot;stone2000&quot;) st = scaling_st(P2,mean(basins[[i]]$y)) res$S2[i] = st$Nneutrons } print(res) plot(res$S2,res$S1,pch=21,bg=cols, xlab=&quot;S2 : Scaling from average elevation&quot;,ylab=&quot;S1 : Scaling pixel by pixel&quot;, xlim=range(0,res$S2,res$S1),ylim=range(0,res$S2,res$S1)) grid() text(res$S2,res$S1,res$names,cex=0.5,pos=1) abline(0,1,lty=2) Comment on the differences. What are the other parameters which can influence the calculation of average production rates at the scale of a basin? 6.4 Change in denudation through time 6.5 On the importance of muons For \\(^{10}\\)Be the muonic represent a very small fraction of surface production : Code nuc = &quot;Be10&quot; f = (prm[2,nuc]+prm[3,nuc])/sum(prm[1:3,nuc])*100 paste(&quot;For&quot;,nuc,&quot;muons represent&quot;,round(f,1),&quot;% of SLHL surface production&quot;) ## [1] &quot;For Be10 muons represent 1.3 % of SLHL surface production&quot; Muons provide a a very small fraction of production at the surface for \\(^{10}\\)Be, but they penetrate much deeper than neutrons into rocks. For that reason they are going to account to a higher proportion of nuclides on eroding surfaces. We can separate the various sources of production (neutrons and muons) and see what is their respective contributions to the concentration observed at the surface. Code col_sp = &quot;deepskyblue&quot; col_sm = &quot;indianred1&quot; col_fm = &quot;darkseagreen1&quot; nuc = &quot;Be10&quot; data = data.frame(ero1=10^seq(log10(0.1),log10(2000),length=100)) # ero1 -&gt; denudation in m/Ma emin = min(data$ero1) emax = max(data$ero1) data$ero2 = data$ero1/1e6*100*rho # ero2 = denudation in g/cm2/a # steady state concentrations associated with individual production pathways data$Csp = prm[1,nuc]*S$Nneutrons/(prm[4,nuc]+(data$ero2/Lambda[1])) data$Csm = prm[2,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[2])) data$Cfm = prm[3,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[3])) data$C = data$Csp + data$Csm + data$Cfm # plot(NA,xlim=c(emin,emax),ylim=c(0,1),log=&quot;x&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Fraction&quot;,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) polygon(c(emin,emax,emax,emin),c(0,0,1,1),col=col_fm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev((data$Csp+data$Csm)/data$C)),col=col_sm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev(data$Csp/data$C)),col=col_sp) grid(col=&quot;black&quot;,equilogs = FALSE) # text(0.2,0.1,nuc,cex=2) legend(&quot;bottomright&quot;, c(&quot;Spallation&quot;,&quot;Stopping muons&quot;,&quot;Fast muons&quot;), pch=22,pt.bg=c(col_sp,col_sm,col_fm),pt.cex=1.5,cex=1,bg=&quot;white&quot;) Below you can get all the necessary code to produce the figure Code library(&quot;TCNtools&quot;) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees rho = 2.7 data(prm) data(Lambda) P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) col_sp = &quot;deepskyblue&quot; col_sm = &quot;indianred1&quot; col_fm = &quot;darkseagreen1&quot; nuc = &quot;Be10&quot; data = data.frame(ero1=10^seq(log10(0.1),log10(2000),length=100)) # ero1 -&gt; denudation in m/Ma emin = min(data$ero1) emax = max(data$ero1) data$ero2 = data$ero1/1e6*100*rho # ero2 = denudation in g/cm2/a # steady state concentrations associated with individual production pathways data$Csp = prm[1,nuc]*S$Nneutrons/(prm[4,nuc]+(data$ero2/Lambda[1])) data$Csm = prm[2,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[2])) data$Cfm = prm[3,nuc]*S$Nmuons/(prm[4,nuc]+(data$ero2/Lambda[3])) data$C = data$Csp + data$Csm + data$Cfm # plot(NA,xlim=c(emin,emax),ylim=c(0,1),log=&quot;x&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Fraction&quot;,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) polygon(c(emin,emax,emax,emin),c(0,0,1,1),col=col_fm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev((data$Csp+data$Csm)/data$C)),col=col_sm) polygon(c(emin,emax,rev(data$ero1)), c(0,0,rev(data$Csp/data$C)),col=col_sp) grid(col=&quot;black&quot;,equilogs = FALSE) # text(0.2,0.1,nuc,cex=2) legend(&quot;bottomright&quot;, c(&quot;Spallation&quot;,&quot;Stopping muons&quot;,&quot;Fast muons&quot;), pch=22,pt.bg=c(col_sp,col_sm,col_fm),pt.cex=1.5,cex=1,bg=&quot;white&quot;) "],["two-nuclides-systems.html", "7 Two nuclides systems 7.1 Evolution of ratios 7.2 Steady-state denudation and constant exposure curves", " 7 Two nuclides systems We first define the usual variables and parameters. Code data(Lambda) # we load a vector containing the attenuation length into the environment data(prm) # we load a matrix containing the production/decay parameters into the environment rho = 2.7 # we also define the density (g/cm3) altitude = 1000 # elevation in m latitude = 45 # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) Here first we consider that all samples are close and have similar positions and elevation, so similar scaling parameters. We will see later how to deal with situations where samples have different positions and hence different scaling parameters. In order to avoid messing up things later we define here once and for all which is nuclide 1 and which is nuclide 2, according to their respective \\(\\tau_{1/2}\\) : Code N1 = &quot;Be10&quot; # longer half-life N2 = &quot;Al26&quot; # shorter half-life 7.1 Evolution of ratios We first start by looking into the evolution of the isotopic ratio between the nuclides, once production has stopped, which is one of the key phenomenon used in burial dating applications. We will look at the evolution through time (typically over several Ma for the 26/10 system), starting from concentrations defined by a denudation steady-state. Code data = data.frame(t = seq(0,7e6,by=2000)) ero = 50 * rho * 100 / 1e6 # in g/cm2/a We compute the initial steady-state concentrations. Code C1_0 = solv_conc_eul(0,ero,Inf,0,prm[,N1],S,Lambda) C2_0 = solv_conc_eul(0,ero,Inf,0,prm[,N2],S,Lambda) Now we let them decay through time starting from the initial values, with no production (\\(z=+\\infty\\)). We plot the evolution of the ratio through time. Code data$C1 = solv_conc_eul(Inf,0,data$t,C1_0,prm[,N1],S,Lambda) data$C2 = solv_conc_eul(Inf,0,data$t,C2_0,prm[,N2],S,Lambda) plot(data$t,data$C2/data$C1,type=&quot;l&quot;,lwd=2,col=&quot;darkorange&quot;,xlab=&quot;Time (a)&quot;,ylab=paste(N2,&quot;/&quot;,N1)) 7.2 Steady-state denudation and constant exposure curves Two-nuclides plots are usually built around two curves representing the predictions, in terms of concentrations, for end-member simplified situations : steady-state denudation, where the concentrations of the nuclides of interest are computed for various values of surface denudation rates constant exposure, where the concentrations of the nuclides of interest are computed for various duration of surface exposition, with no denudation We will use the Eulerian point of view (function solv_conc_eul) to compute the concentrations, which is done by function tnp_curves. We compute the data for these two curves. Code tmp = tnp_curves(prm[,N1],prm[,N2],Lambda,S,rho) ss_ero = tmp[[1]] cst_exp = tmp[[2]] The default parameters for the ranges of denudation rates and exposure ages can be adjusted if needed. Now that we can plot everything in a two-nuclides graph. While not mandatory, it is usually recommended to organize the plot this way : X-axis : longer half-life nuclide (our \\(N_1\\)) Y-axis : ratio of shorter to longer half-life nuclides (our \\(N_2/N_1\\)) Code plot(NA,xlim=range(cst_exp$C1,ss_ero$C1),ylim=range(cst_exp$C2/cst_exp$C1,ss_ero$C2/ss_ero$C1),log=&quot;x&quot;, xlab=paste(N1,&quot;(at/g)&quot;),ylab=paste(N2,&quot;/&quot;,N1)) lines(cst_exp$C1,cst_exp$C2/cst_exp$C1,lty=2,col=&quot;khaki4&quot;) # constant exposure, dashed line lines(ss_ero$C1,ss_ero$C2/ss_ero$C1,col=&quot;khaki4&quot;) # steady-state erosion, solid line lines(data$C1,data$C2/data$C1,lwd=2,col=&quot;darkorange&quot;) We have also plotted the burial trajectory we generated above. Starting from a position on the steady-state denudation curve, we progressively lower the ratios. TODO Change the denudation rate used to compute the initial steayd-state concentrations "],["depth-profiles.html", "8 Depth profiles 8.1 Setup and dataset 8.2 Profile modelling 8.3 Exploration of parameters space 8.4 Interactive shiny app", " 8 Depth profiles 8.1 Setup and dataset The package TCNtools includes a selection of depth profiles data from the literature. We will use the profile published in Laloy et al. (2017) from the Campines Plateau in Belgium. Code data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data We can then compute the scaling for this study site. Note that each site characteristics are only reported for the first sample (label=1). Code altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) We plot the depth profile Code plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce this figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Comment on the evolution of concentrations with depth. 8.2 Profile modelling We are going to use equation (4.1) to compute a theoretical concentration profile. We use initial guess of the value for the parameters Material density \\(\\rho\\) (rho) Surface denudation \\(\\varepsilon\\) (ero) Age of the deposit \\(t\\) (age) Inherited concentration at the time of deposition \\(C_0\\) (C0) Code rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ We now plot this modeled profile with the dataset. Code plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modelled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce this figure Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modelled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Try to get as close to the observations by modifying the various parameters. Comment on the influence of each parameter on the profile. 8.3 Exploration of parameters space We could change the parameters incrementally one by one to explore the effects on the modeled profile and attempt to converge toward a best-looking solution. It is obviously much more efficient and robust to do this automatically by going through a large number of parameters sets, compute the predicted concentrations (\\(C_{mod}\\)) and evaluate how well they match the measured concentrations (\\(C_{mes}\\) with uncertainty \\(\\sigma\\)). We can use the \\(\\chi^2\\) to quantify the difference between the model and observations. \\[\\begin{equation} \\chi^2 = \\sum_{i=1}^n \\left( \\frac{C_{mes,i} - C_{mod,i} }{\\sigma_i} \\right)^2 \\end{equation}\\] The function depth_profile_mc will allow us to generate a large number of models over the parameter space defined by : the age of the formation (age in a) the surface denudation rate (ero in m/Ma) the inherited concentration (inh in at/g) the density (rho in g/cm2) There two additional parameters n1 and n2, which control how many models we are going to sample from this parameters space n1 is the number of model which are randomly drawn (Monte Carlo) n2 is the number of regularly sampled points for each parameter (if all 4 parameters are varying the number of models is n2\\(^4\\)) We can choose to carry out an exploration of the parameters space which completely random (n2=0) or following a regular grid (n1=0), or a combination of both. This first attempt is a crude exploration of the parameter space. Code #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,100)*1000, ero=c(0,50), inh=c(0,1)*1e5, rho=c(1.5,2.5), n1=50e3) Note that in depth_profile_mc we could choose to fix a parameter by passing a single value as argument, instead of a two elements vector, which corresponds to the bounds of the explored interval. We can have a look at the result table. Code summary(res) ## age ero inh rho ## Min. : 1.42 Min. : 0.00193 Min. : 0.35 Min. :1.500 ## 1st Qu.:24522.58 1st Qu.:12.51497 1st Qu.:24765.41 1st Qu.:1.750 ## Median :49603.10 Median :24.95371 Median :49962.32 Median :2.004 ## Mean :49766.41 Mean :25.01723 Mean :49865.75 Mean :2.000 ## 3rd Qu.:74982.46 3rd Qu.:37.45515 3rd Qu.:74843.07 3rd Qu.:2.249 ## Max. :99999.40 Max. :49.99899 Max. :99997.38 Max. :2.500 ## chi2 ## Min. : 13.04 ## 1st Qu.: 99.40 ## Median : 313.00 ## Mean : 464.91 ## 3rd Qu.: 722.06 ## Max. :2198.57 Based on the \\(\\chi2\\) value we can now try to have a look at a subset of our model, which correspond to the best fitting values for this metric. We can start to do that very coarsely by looking at a quantile, for example the best 1%. Code cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] pairs(best[,-5],pch=16,col=adjustcolor(&quot;blue&quot;,0.2)) # we remove the last column (5) which correspond to the chi2 We see numerous tradeoffs between parameters, notably between age and denudation rate. We can select and plot the best model Code imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe res[imin,] Code z = seq(0,max(data$depth),length.out = 40) # calculation depth (g/cm2) C = solv_conc_eul(z*res$rho[imin],res$ero[imin]*100/1e6*res$rho[imin],res$age[imin],res$inh[imin],prm[,&quot;Be10&quot;],S,Lambda) # compute concentration # plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modeled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Below you can get all the necessary code to produce these figures Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,100)*1000, ero=c(0,50), inh=c(0,1)*1e5, rho=c(1.5,2.5), n1=50e3) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] pairs(best[,-5],pch=16,col=adjustcolor(&quot;blue&quot;,0.2)) # we remove the last column (5) which correspond to the chi2 imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe res[imin,] z = seq(0,max(data$depth),length.out = 40) # calculation depth (g/cm2) C = solv_conc_eul(z*res$rho[imin],res$ero[imin]*100/1e6*res$rho[imin],res$age[imin],res$inh[imin],prm[,&quot;Be10&quot;],S,Lambda) # compute concentration # plot(NA,xlim=range(data$C,50e3,200e3),ylim=rev(range(data$depth)), xlab=&quot;10Be concentration (at/g)&quot;,ylab=&quot;Depth below surface (cm)&quot;) grid() lines(C,z,lwd=3) # plotting the modeled profile arrows(data$C-data$C_e,data$depth,data$C+data$C_e,data$depth,length = 0) points(data$C,data$depth,pch=21,cex=2,bg=&quot;pink&quot;) text(data$C,data$depth,data$label,cex=0.7) Change the range of explored parameters Try to remove some samples, and observe the influence on the solution In many situations we have some knowledge about the values or plausible ranges of some parameters. For example the density can be measured at the sampling site, and inheritance can be estimated from the shape of the profile. In our case it seems reasonable to consider that it is close to 90\\(\\times10^3\\) at/g. We can restrict the range of variation or even fix these parameters, which is what we do next, by letting only the age and denudation rate as free parameters. Code #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,200)*1000, ero=c(0,50), inh=90*1e3, rho=2, n1=20000,n2=20) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] plot(best$ero,best$age/1000,pch=16,col=adjustcolor(&quot;blue&quot;,0.2), xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) In this case the results will be much easier to visualize through the construction of a \\(\\chi^2\\) surface. We use the library akima to grid the results (note that we use \\(\\log_{10}(\\chi^2)\\)). Code library(&quot;akima&quot;) spline&lt;-interp(res$ero/max(res$ero),res$age/max(res$age),log10(res$chi2),duplicate=&quot;mean&quot;,nx=100,ny=100) contour(spline$x*max(res$ero),spline$y*max(res$age)/1e3,spline$z,col=&quot;pink&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) # &quot;best&quot; model Below you can get all the necessary code to produce these figures Code library(&quot;TCNtools&quot;) data(Lambda) data(prm) # data(&quot;tcn_depth_profiles&quot;) # import the data set data = tcn_depth_profiles[tcn_depth_profiles$study==&quot;laloy2017bayesian&quot;,] # selecting a particular study data altitude = data$altitude[1] # elevation in m latitude = data$latitude[1] # latitude in degrees P = atm_pressure(alt=altitude,model=&quot;stone2000&quot;) # compute atmospheric pressure at site S = scaling_st(P,latitude) # compute the scaling parameters according to Stone (2000) rho = 1.7 # density (g/cm3) ero = 40*100/1e6*rho # m/Ma -&gt; g/cm2/a age = 0.3e6 # a C0 = 20e3 # inheritance (at/g) z = seq(0,max(data$depth),length.out = 40)*rho # calculation depth (g/cm2) C = solv_conc_eul(z,ero,age,C0,prm[,&quot;Be10&quot;],S,Lambda) # compute concentration´ #data = data[!data$label %in% c(3,5),] # if we want to remove samples 3 and 5 (use with caution) res = depth_profile_mc(data$C,data$C_e,data$depth,prm[,&quot;Be10&quot;],Lambda,S, age=c(0,200)*1000, ero=c(0,50), inh=90*1e3, rho=2, n1=20000,n2=20) cutoff = quantile(res$chi2,0.01) best = res[res$chi2&lt;cutoff,] plot(best$ero,best$age/1000,pch=16,col=adjustcolor(&quot;blue&quot;,0.2), xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) imin = which.min(res$chi2) # position of the lowest chi2 in the dataframe points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) library(&quot;akima&quot;) spline&lt;-interp(res$ero/max(res$ero),res$age/max(res$age),log10(res$chi2),duplicate=&quot;mean&quot;,nx=100,ny=100) contour(spline$x*max(res$ero),spline$y*max(res$age)/1e3,spline$z,col=&quot;pink&quot;,xlab=&quot;Denudation rate (m/Ma)&quot;,ylab=&quot;Age (ka)&quot;) points(res[imin,]$ero,res[imin,]$age/1000,col=&quot;red&quot;) # &quot;best&quot; model Change the values for density and inheritance Repeat the calculations with the other profiles included in the dataset (data(\"tcn_depth_profiles\")) : Siame et al. (2004), Hein et al. (2009), Hidy et al. (2010) How well are the age and denudation rate defined for this profile? 8.4 Interactive shiny app You can also explore profile modelling dynamically using this embedded application, which use the same type of code. "],["references.html", "9 References", " 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
